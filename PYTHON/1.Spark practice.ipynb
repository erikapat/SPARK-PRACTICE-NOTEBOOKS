{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHAPTER TWO: PRACTICE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A process of Spark Core ...\n",
    "\n",
    "All Spark Core processes (and Spark in general) follow, approximately, the\n",
    "same structure.\n",
    "\n",
    "> 1. Setting environment / application / context.\n",
    "\n",
    "> 2. Loading data in RDDs.\n",
    "\n",
    "> 3. Manipulation of RDDs.\n",
    "\n",
    "> 4. Generation of results.\n",
    "\n",
    "> 5. Closure / release of the environment / application / context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Creation\n",
    "\n",
    "Any RDD must be created on a SparkContext (more on what a SparkContext is, would be explained later) using any of the following functions:\n",
    "\n",
    "* sc.parallelize (col): Creates an RDD that distributes the content of “col” between cluster nodes\n",
    "* sc.textFile (path): Creates an RDD on a file in a file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE:\n",
    "\n",
    "Find the mean of 1000 number\n",
    "\n",
    "**Before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value: 499.5\n",
      "Time: 0.23 seconds\n"
     ]
    }
   ],
   "source": [
    "#time start to running\n",
    "import datetime\n",
    "#Calculamos el tiempo de ejecucion\n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.mean(range(1000))\n",
    "print(\"Mean value: \" + str(x))\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time: \" + str(timedelta) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value: 499.5\n",
      "Time: 1.42 seconds\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#time start to running\n",
    "import datetime\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'standalone-app'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "#Calculamos el tiempo de ejecucion\n",
    "timestart= datetime.datetime.now()\n",
    "rdd = sc.parallelize(range(1000), 10) # 10 number of partitions\n",
    "print(\"Mean value: \" + str(rdd.mean()))\n",
    "\n",
    "# Stop the spark context\n",
    "sc.stop()\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time: \" + str(timedelta) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE 2\n",
    "\n",
    "Create a rrd over a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'interactive-script'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/erikapat/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "# Application code\n",
    "textFile = sc.textFile('README.md') \n",
    "\n",
    "textFile.count()\n",
    "\n",
    "filteredLines = textFile.filter(lambda line: \"Spark\" in line)\n",
    "filteredLines.count()\n",
    "\n",
    "# Stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on RDDs\n",
    "\n",
    "There are two types of operations that can be performed on an RDD:\n",
    "\n",
    "**Transformations:** receive an RDD as input and generate a modified RDD to the\n",
    "     exit.\n",
    "\n",
    "**Actions:** receive an RDD as input, perform some type of computation on the\n",
    "     same and generate as a result a resulting value or an operation of\n",
    "     persistence / saving of results.\n",
    "    \n",
    "* The key to Spark's speed is that ONLY SHARES produce\n",
    "“Really” an execution distributed in the cluster à Lazy evaluation.\n",
    "* Spark is responsible (automatically) for organizing the execution of operations on a\n",
    "RDD so that:\n",
    "* Memory usage is minimized (e.g. parse + filtering vs. filtering + parsing).\n",
    "* Maximize the reuse of data already loaded into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'rdd-creation'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/home/erikapat/Dropbox/conento/conento_prueba/PRUEBA DATA ARQUITECT/'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD TRANSFORMATION \n",
    "\n",
    "# Transformaciones\n",
    "\n",
    "Una transformación se ejecuta sobre un RDD y genera otro RDD a la salida.\n",
    "Se pueden encadenar y se ejecutan en el orden óptimo al solicitar la primera acción.\n",
    "\n",
    "**Transformaciones mediante aplicación de una función de usuario:**\n",
    "\n",
    "* rdd.map(func): Aplica “func” a cada elemento del RDD (un resultado por elemento)\n",
    "* rdd.flatMap(func): Aplica “func” a cada elemento del RDD y “aplana” el resultado\n",
    "* rdd.filter(func): Filtra aquellos elementos del RDD para los que “func” devuelve False\n",
    "* rdd.groupBy(func): Devuelve los elementos de RDD agrupados según el resultado de “func”\n",
    "* rdd.sortBy(func, asc): Devuelve los elementos de RDD ordenador según el resultado de “func”\n",
    "\n",
    "**Transformaciones basadas en funciones de conjunto**\n",
    "\n",
    "* rdd.union(rdd): Devuelve la unión de dos RDD\n",
    "* rdd.intersection(rdd): Devuelve la intersección de dos RDD\n",
    "* rdd.cartesian(rdd): Devuelve el producto cartesiano de los elementos de dos RDD\n",
    "\n",
    "**Otras transformaciones**\n",
    "\n",
    "* rdd.distinct() Devuelve los elementos únicos del RDD (no preserva el orden)\n",
    "* rdd.sample(rep, prob, seed) Devuelve una muestra del RDD aplicando los par‡metros establecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'a', 'c']\n",
      "[('a', 1), ('b', 1), ('c', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'rdd-transformations'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/home/erikapat/Dropbox/conento/conento_prueba/PRUEBA DATA ARQUITECT/'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "# Transformation: map\n",
    "rdd1 = sc.parallelize(['a', 'b', 'c', 'a', 'c'])\n",
    "rdd2 = rdd1.map(lambda element: (element, 1))\n",
    "print(rdd1.collect())\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones\n",
    "\n",
    "Una acción se ejecuta sobre un RDD y produce:\n",
    "\n",
    "* La ejecución “real” de todas las transformaciones previas sobre los RDD involucrados.\n",
    "* La obtención de un resultado (RDD, objeto o fichero) tras la aplicación de una función\n",
    "(de usuario o no) sobre el RDD resultante de todas las transformaciones previas.\n",
    "\n",
    "**Acciones de aplicación de una función de usuario**\n",
    "\n",
    "* rdd.reduce(func): Agrega el RDD aplicando “func” a cada par de elementos (recursivamente)\n",
    "\n",
    "\n",
    "**Acciones de aplicación de funciones matemáticas (“reduce predefinido”)**\n",
    "\n",
    "* rdd.count(): Cuenta el número de elementos en el RDD\n",
    "* rdd.max(): Devuelve el máximo de los elementos incluidos en el RDD\n",
    "* rdd.min(): Devuelve el mínimo de los elementos incluidos en el RDD\n",
    "* rdd.sum(): Devuelve la suma de los elementos del RDD\n",
    "* rdd.mean(): Devuelve la media de los elementos del RDD\n",
    "* rdd.stdev(): Devuelve la desviación estándar de los elementos del RDD\n",
    "* rdd.variance(): Devuelve la varianza de los elementos del RDD\n",
    "\n",
    "**Acciones de conversión de datos Spark a Python**\n",
    "\n",
    "* rdd.collect() Devuelve una colección con todos los elementos del RDD\n",
    "* rdd.take(n) Devuelve una colección con los n primeros elementos del RDD\n",
    "* rdd.first() Devuelve el primer elemento del RDD\n",
    "\n",
    "**Acciones de persistencia**\n",
    "\n",
    "rdd.saveAsTextFile(path) Guarda el RDD como ficheros de texto plano en “path” (directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'rdd-actions'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/home/erikapat/Dropbox/conento/conento_prueba/PRUEBA DATA ARQUITECT/'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action: reduce\n",
    "rdd1 = sc.parallelize([('a', 1), ('b', 2), ('c', 3)])\n",
    "rdd1.reduce(lambda el1, el2: (el1[0] + el2[0], el1[1] + el2[1]))\n",
    "\n",
    "\n",
    "\n",
    "# Actions: count, max, min, sum, mean, stdev, variance\n",
    "rdd1 = sc.parallelize(list(range(100)))\n",
    "rdd1.count()\n",
    "rdd1.max()\n",
    "rdd1.min()\n",
    "rdd1.sum()\n",
    "rdd1.mean()\n",
    "rdd1.stdev()\n",
    "rdd1.variance()\n",
    "\n",
    "\n",
    "# Actions: collect, take, first\n",
    "rdd1 = sc.parallelize([('a', 1), ('c', 10), ('b', 2)])\n",
    "rdd1.collect()\n",
    "rdd1.take(1)\n",
    "rdd1.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actions: saveAsTextFile\n",
    "rdd1 = sc.parallelize([('a', 1), ('c', 10), ('b', 2)])\n",
    "rdd1.saveAsTextFile('/home/erikapat/Dropbox/conento/conento_prueba/PRUEBA DATA ARQUITECT/output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs\n",
    "\n",
    "* Un Pair RDD es un tipo especial de RDD en el que la información almacenada tiene la\n",
    "estructura clave-valor (en Python una tupla/lista de 2 elementos)\n",
    "* Se pueden crear mediante la aplicación de un map inicial a un RDD en el que se crea la\n",
    "estructura clave-valor manualmente (se devuleve una tupla/lista por cada elemento).\n",
    "* Todas las transformaciones y acciones disponibles para RDDs pueden usarse también\n",
    "para Pair RDDs (teniendo cuidado en que ahora cada elemento es una tupla/lista).\n",
    "* Adicionalmente, Spark incluye algunas transformaciones y acciones específicas que sólo\n",
    "aplican a Pair RDDs\n",
    "\n",
    "\n",
    "## Transformaciones (sólo Pair RDDs)**\n",
    "\n",
    "**Transformaciones basadas en operaciones por clave**\n",
    "\n",
    "* rdd.sortByKey(asc) Devuelve el RDD con los elementos ordenados por clave\n",
    "* rdd.reduceByKey(func) Devuleve un RDD resultante de aplicar “func” a los elementos de cada clave\n",
    "\n",
    "**Transformaciones basadas en cruces**\n",
    "\n",
    "* rdd.join(rdd) Sobre (K, V) y (K, W) devuelve (K, (V, W)) para los K existentes en ambos RDD\n",
    "* rdd.leftOuterJoin(rdd) join pero manteniendo los K que estén en el primer RDD y no en el segundo\n",
    "* rdd.rightOuterJoin(rdd) join pero manteniendo los K que estén en el segundo RDD y no en el primero\n",
    "* rdd.fullOuterJoin(rdd) join pero manteniendo todos los K aunque no haya coincidencia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2014', ['2014-12-31', '2014-08-06']), ('2015', ['2015-01-25']), ('2017', ['2017-01-05']), ('2016', ['2016-05-17', '2016-11-08'])]\n",
      "[('2014', ['2014-12-31', '2014-08-06']), ('2015', ['2015-01-25']), ('2016', ['2016-05-17', '2016-11-08']), ('2017', ['2017-01-05'])]\n",
      "[('2014', '12-31'), ('2015', '01-25'), ('2016', '05-17'), ('2016', '11-08'), ('2017', '01-05'), ('2014', '08-06')]\n",
      "[('2014', '12-31'), ('2015', '01-25'), ('2016', '11-08'), ('2017', '01-05')]\n",
      "[(2014, (10000, -500)), (2016, (30000, -6000))]\n",
      "[(2014, (10000, -500)), (2015, (20000, None)), (2016, (30000, -6000))]\n",
      "[(2014, (10000, -500)), (2016, (30000, -6000)), (2017, (None, -9000))]\n",
      "[(2014, (10000, -500)), (2015, (20000, None)), (2016, (30000, -6000)), (2017, (None, -9000))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'pair-rdd-transformations'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "\n",
    "\n",
    "# Transformation: sortByKey\n",
    "rdd1 = sc.parallelize(['2014-12-31', '2015-01-25', '2016-05-17', '2016-11-08', '2017-01-05', '2014-08-06'])\n",
    "rdd2 = rdd1.groupBy(lambda element: element.split('-')[0])\n",
    "print([(key, list(value)) for (key, value) in rdd2.collect()])\n",
    "rdd3 = rdd2.sortByKey()\n",
    "print([(key, list(value)) for (key, value) in rdd3.collect()])\n",
    "\n",
    "\n",
    "# Transformation: reduceByKey\n",
    "rdd1 = sc.parallelize(['2014-12-31', '2015-01-25', '2016-05-17', '2016-11-08', '2017-01-05', '2014-08-06'])\n",
    "\n",
    "def parseDate(date):\n",
    "    year, month, day = date.split('-')\n",
    "    return (year, month + '-' + day)\n",
    "\n",
    "rdd2 = rdd1.map(parseDate)\n",
    "\n",
    "def maxDate(date1, date2):\n",
    "    if date1 > date2:\n",
    "        return date1\n",
    "    else:\n",
    "        return date2\n",
    "    \n",
    "rdd3 = rdd2.reduceByKey(maxDate).sortByKey()\n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())\n",
    "\n",
    "\n",
    "\n",
    "# Transformation: join, leftOuterJoin, rightOuterJoin, fullOuterJoin\n",
    "rdd1 = sc.parallelize([(2014, 10000), (2015, 20000), (2016, 30000)])\n",
    "rdd2 = sc.parallelize([(2014, -500), (2016, -6000), (2017, -9000)])\n",
    "\n",
    "rdd3 = rdd1.join(rdd2).sortByKey()\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd3 = rdd1.leftOuterJoin(rdd2).sortByKey()\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd3 = rdd1.rightOuterJoin(rdd2).sortByKey()\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd3 = rdd1.fullOuterJoin(rdd2).sortByKey()\n",
    "print(rdd3.collect())\n",
    "\n",
    "# Stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones (sólo Pair RDDs)\n",
    "\n",
    "**Acciones basadas en operaciones por clave**\n",
    "\n",
    "* rdd.countByKey() Cuenta el número de elementos en cada una de las claves\n",
    "\n",
    "**Acciones de extracción de elementos del Pair RDD**\n",
    "\n",
    "* rdd.keys() Obtiene un RDD a partir de las claves del RDD original\n",
    "* rdd.values() Obtiene un RDD a partir de los valores del RDD original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014', '2015', '2016', '2016', '2017', '2014']\n",
      "['12-31', '01-25', '05-17', '11-08', '01-05', '08-06']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'pair-rdd-actions'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "\n",
    "\n",
    "# Action: countByKey\n",
    "rdd1 = sc.parallelize(['2014-12-31', '2015-01-25', '2016-05-17', '2016-11-08', '2017-01-05', '2014-08-06'])\n",
    "\n",
    "def parseDate(date):\n",
    "    year, month, day = date.split('-')\n",
    "    return (year, month + '-' + day)\n",
    "\n",
    "rdd2 = rdd1.map(parseDate)\n",
    "rdd2.countByKey()\n",
    "\n",
    "\n",
    "\n",
    "# Action: keys, values\n",
    "rdd3 = rdd2.keys()\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd4 = rdd2.values()\n",
    "print(rdd4.collect())\n",
    "\n",
    "\n",
    "# Stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables compartidas\n",
    "\n",
    "Imaginemos el siguiente código...\n",
    "\n",
    "`counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "def increment_counter(x):\n",
    "global counter\n",
    "counter += x\n",
    "rdd.map(increment_counter)\n",
    "print(\"Counter value: \", counter)`\n",
    "\n",
    "¿Qué valor tendrá “counter” al finalizar la ejecución del mismo sobre un cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables compartidas\n",
    "* NO podemos utilizar variables locales en funciones ejecutadas de forma distribuida en\n",
    "Spark.\n",
    "* Aunque el resultado puede ser correcto ejecutado en local, no tiene por qué serlo al\n",
    "ejecutarlo en el cluster.\n",
    "* Spark pone a nuestra disposición dos herarmientas que nos permiten trabajar con “variables\n",
    "compartidas” entre nodos del cluster:\n",
    "*Accumulators.\n",
    "* Broadcast variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "\n",
    "* Un accumulator es una variable numérica visible y accesible desde todos los nodos de un\n",
    "cluster involucrados en la ejecución de un proceso.\n",
    "* Sobre esta variable numérica únicamente se pueden ejecutar operaciones de incremento,\n",
    "por lo que podremos utilizarlos para almacenar:\n",
    "* Contadores.\n",
    "* Sumas.\n",
    "\n",
    "**Casos de uso:**\n",
    "* Debug: contar “maps” en los que se da una situación específíca.\n",
    "* Traza: número total de registros procesados hasta un momento dado.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de accumulators**\n",
    "\n",
    "* sc.accumulator(initial) Crea un accumulator y lo inicializa al valor de “initial”\n",
    "\n",
    "**Incremento de accumulators**\n",
    "\n",
    "* accum.add(value) Incrementa el valor del accumulator en “value”\n",
    "\n",
    "**Recuperación del valor de accumulators**\n",
    "\n",
    "* accum.value Recupera el valor actual de un accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variables\n",
    "* Una variable broadcast permite crear una variable de sólo lectura accesible desde todos\n",
    "los nodos de un cluster involucrados en la ejecución de un proceso.\n",
    "* Son útiles cuando tenemos algún dato / estructura (no excesivamente grande) necesaria\n",
    "para la ejecución de tareas map / reduce de forma distribuida.\n",
    "* Evitan tener que distribuir y parsear datos comunes a todas las tareas.\n",
    "\n",
    "Casos de uso:\n",
    "* Tareas “dependientes” de resultados previos: valores de filtrado a partir de resultados de\n",
    "map/reduce previos.\n",
    "* Tablas de asociación: traducción de códigos a descripciones.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de broadcast variables\n",
    "\n",
    "* sc.broadcast(var) Crea una variable compartida a partir del valor “local” de “var”\n",
    "Recuperación del valor de broadcast variables\n",
    "\n",
    "* broadcastVar.value Recupera el valor de una variable compartida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[('Cristina', 'Lengua', 10), ('Cristina', 'Matematicas', 8), ('Lucia', 'Lengua', 7), ('Lucia', 'Matematicas', 9)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'pair-rdd-actions'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "\n",
    "\n",
    "# Accumulators\n",
    "errors = sc.accumulator(0)\n",
    "\n",
    "def parseDate(date):\n",
    "    try:\n",
    "        year, month, day = date.split('-')\n",
    "        return (year, month + '-' + day)\n",
    "    except:\n",
    "        errors.add(1)\n",
    "\n",
    "rdd1 = sc.parallelize(['2014-12-31', '2015-01-25', '2016-05-17', '2016-', '2017-01-05', '2014-06'])\n",
    "rdd2 = rdd1.map(parseDate)\n",
    "results = rdd2.collect()\n",
    "\n",
    "print(errors.value)\n",
    "\n",
    "\n",
    "# Broadcast variables\n",
    "students = dict()\n",
    "students['25'] = 'Cristina'\n",
    "students['12'] = 'Lucia'\n",
    "studentsBC = sc.broadcast(students)\n",
    "\n",
    "subjects = dict()\n",
    "subjects['0'] = 'Lengua'\n",
    "subjects['1'] = 'Matematicas'\n",
    "subjectsBC = sc.broadcast(subjects)\n",
    "\n",
    "rdd1 = sc.parallelize([(25, 0, 10), (25, 1, 8), (12, 0, 7), (12, 1, 9)])\n",
    "\n",
    "def translate(element):\n",
    "    return (studentsBC.value[str(element[0])], subjectsBC.value[str(element[1])], element[2])\n",
    "\n",
    "rdd2 = rdd1.map(translate)\n",
    "print(rdd2.collect())\n",
    "\n",
    "\n",
    "# Stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORDEN DE ANÁLISIS \n",
    "\n",
    "\n",
    "1.- Carga y tratamiento inicial: Spark Core.\n",
    "\n",
    "2.- Consulta y análisis exploratorio: Spark SQL.\n",
    "\n",
    "3.- Machine learning: MLlib.\n",
    "\n",
    "4.- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext vs. SparkSession\n",
    "* Para poder hacer uso de Spark Core, lo primero que hay que hacer es crear un objeto de\n",
    "tipo SparkContext.\n",
    "* Para poder hacer uso de Spark SQL, lo primero que hay que hacer es crear un objeto de\n",
    "tipo SparkSession.\n",
    "* Spark SQL está construido “sobre” Spark Core, por lo que un objeto de tipo SparkSession\n",
    "SIEMPRE contiene un objeto de tipo SparkContext.\n",
    "* En versiones anteriores de Spark había que crear explícitamente un SparkContext y, luego,\n",
    "pasarlo como parámetro en la creación de un SQLContext o HiveContext.\n",
    "* A partir de la versión 2.0, SparkSession nos da un único punto de acceso a todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'spark-session'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PYSPARK AND SQL QUERIES**\n",
    "\n",
    "https://dataplatform.cloud.ibm.com/exchange/public/entry/view/972c1d5333a2d12ccdbd2437298e8567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Creación de DataFrames\n",
    "\n",
    "* spark.createDataFrame(rdd,[schema]) Creación de un DataFrame a partir de un RDD (con esquema\n",
    "opcional)\n",
    "* spark.read.[format](<path>) Creación de un DataFrame mediante lectura directa de un fichero\n",
    "de datos “estructurado”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'dataframe-creation'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql.types import Row, StringType, LongType, StructField, StructType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|amount|country|\n",
      "+------+-------+\n",
      "| 15823|     US|\n",
      "|  6859|     US|\n",
      "| 17906|     US|\n",
      "| 67081|     US|\n",
      "| 32772|     RW|\n",
      "|  2065|     US|\n",
      "|577844|     US|\n",
      "|  4952|     US|\n",
      "| 45959|     US|\n",
      "|214035|     US|\n",
      "+------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- amount: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################\n",
    "# Infer the schema from Row RDD\n",
    "###############################\n",
    "\n",
    "# Read the file\n",
    "kickstarter = sc.textFile('/home/erikapat/Dropbox/Spark/spark-sql/labs/data/live.tsv')\n",
    "\n",
    "# Function used to parse and transform to Row each line\n",
    "def parseKickstarter(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(country = fields[3], amount = fields[1])\n",
    "\n",
    "# Transform RDD into RDD of Rows\n",
    "row_kickstarter = kickstarter.map(parseKickstarter)\n",
    "row_kickstarter.take(10)\n",
    "[(row['country'], row['amount']) for row in row_kickstarter.take(10)]\n",
    "\n",
    "# Convert Row RDD into DataFrame\n",
    "kick_df = spark.createDataFrame(row_kickstarter)\n",
    "\n",
    "# Check type and contents\n",
    "type(kick_df)\n",
    "kick_df.show(10)\n",
    "kick_df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# Explicitly specify the schema\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sobre DataFrames\n",
    "Una vez disponemos de un conjunto de datos “estructurado”, Spark SQL pone a nuestra\n",
    "disposición un abanico amplio de operaciones para trabajar sobre el mismo.\n",
    "* Estas operaciones cubrirán las mismas funcionalidades que se pueden hacer con este tipo\n",
    "de estructuras en SQL (al igual que en R y Python).\n",
    "* En concreto:\n",
    " Inspección de estructura: entender cómo están organizados los datos.\n",
    "    \n",
    " Inspección de contenido: visualizar los valores concretos de los datos.\n",
    "    \n",
    " Consulta: incluyendo dentro de esta consulta...\n",
    "    \n",
    "* Selección de columnas.\n",
    "* Filtrado.\n",
    "* Ordenación.\n",
    "* Agrupación y agregación.\n",
    "* Unión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de estructura\n",
    "\n",
    "En primer lugar, Spark SQL pone a nuestra disposición herramientas suficientes para poder\n",
    "tener una visión clara de la estructura que siguen nuestros datos:\n",
    "* Nombres de campos.\n",
    "* Tipos.\n",
    "* ...\n",
    "\n",
    "#### Inspección de estructura\n",
    "\n",
    "* df.dtypes Listado de columnas con nombre y tipo\n",
    "* df.schema Visualización del esquema como StructType\n",
    "* df.printSchema() Visualización “formateada” del esquema\n",
    "* df.columns Listado de columnas del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de contenido\n",
    "\n",
    "\n",
    "* df.show(n) Muestra n filas en formato tabla\n",
    "* df.head(n) Muestra n filas como lista de objetos Row\n",
    "* df.take(n) Muestra n filas como lista de objetos Row\n",
    "* df.first() Muestra la primera fila como objeto Row\n",
    "* df.count() Recuperación del número de filas\n",
    "* df.distinct() Recuperación de las filas únicas\n",
    "* df.describe() Cálculo de estadísticos básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'dataframe-inspection'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "# Read the file\n",
    "kickstarter = sc.textFile('/home/erikapat/Dropbox/Spark/spark-sql/labs/data/live.tsv')\n",
    "\n",
    "# Function used to parse and transform to Row each line\n",
    "def parseKickstarter(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(country = fields[3], amount = int(fields[1]))\n",
    "\n",
    "# Transform RDD into RDD of Rows\n",
    "row_kickstarter = kickstarter.map(parseKickstarter)\n",
    "\n",
    "# Convert Row RDD into DataFrame\n",
    "kick_df = spark.createDataFrame(row_kickstarter)\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# Structure inspection\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas sobre DataFrames\n",
    "\n",
    "* Spark SQL pone a nuestra disposición dos formas muy distintas de consultar el contenido de DataFrames:\n",
    "\n",
    "** Mediante la utilización de funciones especificas de consulta.\n",
    "\n",
    "** Mediante la ejecución de consultas SQL.\n",
    "\n",
    "* En el caso de las funciones específicas de consulta, Spark SQL nos ofrece una función\n",
    "para cada posible fragmento (clause) de una query SQL. Así, tendremos funciones\n",
    "específicas para selección de columnas, filtrado, ordenación, agregación...\n",
    "* En el caso de SQL, únicamente tendremos que “registrar” nuestro(s) DataFrame(s) como\n",
    "tabla/vista lógica de datos y podremos utilizar sintáxis SQL (casi completa) para definir\n",
    "nuestras consultas.\n",
    "* Inicialmente, Spark SQL sólo ofrecía funciones específicas, pero, poco a poco, ha ido dando\n",
    "cada vez más soporte a la utilización directa de SQL (por motivos obvios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencia a columnas\n",
    "\n",
    "* df.<column_name> Referencia (sólo referencia) a columna como atributo\n",
    "* df[‘<column_name>’] Referencia (sólo referencia) a columna por nombre7\n",
    "\n",
    "## Referencia a columnas\n",
    "\n",
    "* df.<column_name> Referencia (sólo referencia) a columna como atributo\n",
    "* df[‘<column_name>’] Referencia (sólo referencia) a columna por nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'amount'>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'dataframe-query'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "# Read the file\n",
    "kickstarter = sc.textFile('/home/erikapat/Dropbox/Spark/spark-sql/labs/data/live.tsv')\n",
    "\n",
    "# Function used to parse and transform to Row each line\n",
    "def parseKickstarter(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(country = fields[3], amount = int(fields[1]))\n",
    "\n",
    "# Transform RDD into RDD of Rows\n",
    "row_kickstarter = kickstarter.map(parseKickstarter)\n",
    "\n",
    "# Convert Row RDD into DataFrame\n",
    "kick_df = spark.createDataFrame(row_kickstarter)\n",
    "\n",
    "\n",
    "###############################\n",
    "# Referencing columns\n",
    "###############################\n",
    "\n",
    "# As attributes\n",
    "kick_df.amount\n",
    "\n",
    "# As indexes\n",
    "kick_df['amount']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas sobre DataFrames basadas en SQL\n",
    "\n",
    "* df.createTempView(<name>) Registra el DataFrame con el nombre <name> para su posterior\n",
    "uso como tabla SQL\n",
    "* df.createOrReplaceTempView(<name>) Registra o sustituye el DataFrame con el nombre <name> para\n",
    "su posterior uso como tabla SQL\n",
    "* spark.sql(<query>) Ejecuta la <query> sobre las tablas registradas en la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|amount_most|         location|\n",
      "+-----------+-----------------+\n",
      "|    8782571|  Los Angeles, CA|\n",
      "|    6465690|       Denver, CO|\n",
      "|    5408916|  Los Angeles, CA|\n",
      "|    5702153|    San Diego, CA|\n",
      "|    3336371|San Francisco, CA|\n",
      "|   20338986|    Palo Alto, CA|\n",
      "|    4188927|Newport Beach, CA|\n",
      "|    3986929|       Irvine, CA|\n",
      "|    2090104|        Derby, UK|\n",
      "|    3007370|  Toronto, Canada|\n",
      "|    6333295|     Tokyo, Japan|\n",
      "|   10266845|    Palo Alto, CA|\n",
      "|    3845170|     Tokyo, Japan|\n",
      "|   12779843| Redwood City, CA|\n",
      "|    5545991|     Tokyo, Japan|\n",
      "|    3246588|       Dallas, TX|\n",
      "|    8596474|  Los Angeles, CA|\n",
      "|   13285226|     Portland, OR|\n",
      "|    2933252|Newport Beach, CA|\n",
      "|    5764229|  Minneapolis, MN|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+\n",
      "|length(location)|\n",
      "+----------------+\n",
      "|              15|\n",
      "|              10|\n",
      "|              15|\n",
      "|              13|\n",
      "|              17|\n",
      "|              13|\n",
      "|              17|\n",
      "|              10|\n",
      "|               9|\n",
      "|              15|\n",
      "|              12|\n",
      "|              13|\n",
      "|              12|\n",
      "|              16|\n",
      "|              12|\n",
      "|              10|\n",
      "|              15|\n",
      "|              12|\n",
      "|              17|\n",
      "|              15|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----------+\n",
      "|            location|amount_live|\n",
      "+--------------------+-----------+\n",
      "|      Wilmington, DE|     577844|\n",
      "|     Los Angeles, CA|     214035|\n",
      "|     Miami Beach, FL|     110431|\n",
      "|   San Francisco, CA|     192915|\n",
      "|Lausanne, Switzer...|     123660|\n",
      "|     Wroclaw, Poland|     194009|\n",
      "|      Washington, DC|     334324|\n",
      "| Copenhagen, Denmark|     218565|\n",
      "|         Phoenix, AZ|     272841|\n",
      "|   San Francisco, CA|     933349|\n",
      "| Ljubljana, Slovenia|     238114|\n",
      "|     Los Angeles, CA|     236963|\n",
      "|     Toronto, Canada|     206691|\n",
      "|       Cambridge, MA|     368580|\n",
      "|         Bristol, RI|     296979|\n",
      "|   Wolverhampton, UK|     104316|\n",
      "|     Los Angeles, CA|     198896|\n",
      "|  Salt Lake City, UT|     180642|\n",
      "|      Beijing, China|     439909|\n",
      "|   San Francisco, CA|     318367|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------------+\n",
      "|count(1)|max(amount_most)|\n",
      "+--------+----------------+\n",
      "|     288|         8782571|\n",
      "+--------+----------------+\n",
      "\n",
      "+--------+----------------+\n",
      "|count(1)|max(amount_most)|\n",
      "+--------+----------------+\n",
      "|     288|         8782571|\n",
      "+--------+----------------+\n",
      "\n",
      "+-------------------+----------------+----------------+\n",
      "|           location|max(amount_live)|max(amount_most)|\n",
      "+-------------------+----------------+----------------+\n",
      "|     Pleasanton, CA|            7508|          370280|\n",
      "|       Richmond, VA|           11692|          248253|\n",
      "|         Tucson, AZ|           11010|          310348|\n",
      "|Montpellier, France|             353|          801057|\n",
      "|    Jersey City, NJ|           22133|          103297|\n",
      "|    Santiago, Chile|               0|          442313|\n",
      "|        Redding, CA|            4949|          132500|\n",
      "|         Queens, NY|             904|          151562|\n",
      "|          Derby, UK|             155|         2090104|\n",
      "|        Fairfax, VA|           97788|         2232933|\n",
      "|       Kennesaw, GA|           60727|          243945|\n",
      "|     Quebec, Canada|           31917|          120129|\n",
      "|       Auckland, NZ|           38620|          295138|\n",
      "| West Hollywood, CA|               0|          233228|\n",
      "|     Costa Mesa, CA|            7859|          240672|\n",
      "|      Edinburgh, UK|            6165|          493795|\n",
      "|       Oslo, Norway|           42315|         1538425|\n",
      "|     Plantation, FL|             150|           94904|\n",
      "|      Ridgeland, MS|           11039|          355970|\n",
      "|         Aurora, CO|           29044|          346071|\n",
      "+-------------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration properties\n",
    "APP_NAME = 'dataframe-sql'\n",
    "MASTER = 'local[*]'\n",
    "SPARK_HOME = '/Users/miguel/Documents/Spark/spark'\n",
    "PYSPARK_LIB = 'pyspark.zip'\n",
    "PY4J_LIB = 'py4j-0.10.4-src.zip'\n",
    "\n",
    "# Spark initialization\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PYSPARK_LIB))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', PY4J_LIB))\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Read files\n",
    "live = sc.textFile('/home/erikapat/Dropbox/Spark/spark-sql/labs/data/live.tsv')\n",
    "most = sc.textFile('/home/erikapat/Dropbox/Spark/spark-sql/labs/data/most-backed.tsv')\n",
    "\n",
    "# Functions used to parse and transform to Row each line\n",
    "def parseLive(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(location = fields[6], amount_live = int(fields[1]))\n",
    "\n",
    "def parseMost(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(location = fields[6], amount_most = int(fields[1]))\n",
    "\n",
    "# Transform RDD into RDD of Rows\n",
    "row_live = live.map(parseLive)\n",
    "row_most = most.map(parseMost)\n",
    "\n",
    "# Convert Row RDD into DataFrame\n",
    "live_df = spark.createDataFrame(row_live)\n",
    "most_df = spark.createDataFrame(row_most)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Registring dataframe as SQL table\n",
    "###################################\n",
    "\n",
    "# Register new table\n",
    "live_df.createTempView('live')\n",
    "\n",
    "# Register/Update existing table\n",
    "live_df.createOrReplaceTempView('live')\n",
    "most_df.createOrReplaceTempView('most')\n",
    "\n",
    "\n",
    "###################################\n",
    "# Execute SQL queries\n",
    "###################################\n",
    " \n",
    "# Execute queries using (almost) complete SQL\n",
    "spark.sql('SELECT * FROM most').show()\n",
    "\n",
    "spark.sql('SELECT LENGTH(location) FROM most').show()\n",
    "\n",
    "spark.sql('SELECT location, amount_live FROM live WHERE amount_live > 100000').show()\n",
    "\n",
    "spark.sql('SELECT COUNT(*), MAX(amount_most) FROM most WHERE location LIKE \\'%Los Angeles%\\'').show()\n",
    "\n",
    "spark.sql('SELECT COUNT(*), MAX(amount_most) FROM most WHERE location LIKE \\'%Los Angeles%\\'').show()\n",
    "\n",
    "spark.sql('''SELECT live.location, MAX(amount_live), MAX(amount_most) \n",
    "             FROM live, most\n",
    "             WHERE live.location = most.location\n",
    "             GROUP BY live.location''').show()\n",
    "\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De Spark a Python\n",
    "\n",
    "* df.rdd Convierte un DataFrame de Spark en un RDD de Rows\n",
    "* df.toPandas() Convierte un DataFrame de Spark en uno de pandas\n",
    "Persistencia\n",
    "\n",
    "* df.write.[format](<path>) Salva un DataFrame a fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK STRAMING\n",
    "\n",
    "https://github.com/clumdee/Python-and-Spark-for-Big-Data-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
