{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PYSPARK MEMORY MANAGEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056320"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# load data\n",
    "modelDataFile = \"../data/bank-transactions/trans.asc\"\n",
    "\n",
    "CV_data = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .option(\"delimiter\", \";\") \\\n",
    "  .load(modelDataFile)\n",
    "\n",
    "CV_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+\n",
      "|trans_id|account_id|  date|  type|operation|amount|balance|k_symbol|bank|account|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+\n",
      "|  695247|      2378|930101|PRIJEM|    VKLAD| 700.0|  700.0|    null|null|   null|\n",
      "|  171812|       576|930101|PRIJEM|    VKLAD| 900.0|  900.0|    null|null|   null|\n",
      "|  207264|       704|930101|PRIJEM|    VKLAD|1000.0| 1000.0|    null|null|   null|\n",
      "| 1117247|      3818|930101|PRIJEM|    VKLAD| 600.0|  600.0|    null|null|   null|\n",
      "|  579373|      1972|930102|PRIJEM|    VKLAD| 400.0|  400.0|    null|null|   null|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change types of the data\n",
    "from pyspark.sql.functions import col , column\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "CV_data = CV_data.withColumn(\"date\", col(\"date\").cast(\"string\"))\n",
    "#transform string to date\n",
    "from pyspark.sql.functions import to_date\n",
    "CV_data = CV_data.withColumn(\"date_time\", to_date(from_unixtime(unix_timestamp('date', 'yymmdd'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "|trans_id|account_id|  date|  type|operation|amount|balance|k_symbol|bank|account| date_time|partition_id|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "|  695247|      2378|930101|PRIJEM|    VKLAD| 700.0|  700.0|    null|null|   null|1993-01-01|  1993-01-31|\n",
      "|  171812|       576|930101|PRIJEM|    VKLAD| 900.0|  900.0|    null|null|   null|1993-01-01|  1993-01-31|\n",
      "|  207264|       704|930101|PRIJEM|    VKLAD|1000.0| 1000.0|    null|null|   null|1993-01-01|  1993-01-31|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------------\n",
    "#only year\n",
    "#-----------------------\n",
    "#import pyspark.sql.functions as f\n",
    "#CV_data = CV_data.withColumn('year',f.year(f.to_timestamp('date', 'dd/MM/yyyy')))\n",
    "#-----------------------\n",
    "# last day of the month\n",
    "# ----------------------\n",
    "from pyspark.sql.functions import last_day\n",
    "CV_data = CV_data.withColumn('partition_id', last_day(CV_data.date_time))\n",
    "CV_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll partitionated this data to year-month to be saved and loaded. Clearly, we have incompleted data (just transactions for january), but this will be useful anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|partition_id| count|\n",
      "+------------+------+\n",
      "|  1993-01-31| 28205|\n",
      "|  1994-01-31| 91628|\n",
      "|  1995-01-31|133022|\n",
      "|  1996-01-31|196779|\n",
      "|  1997-01-31|284409|\n",
      "|  1998-01-31|322277|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_data.groupBy('partition_id').count().sort('partition_id').show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save and load partitioned tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the memory use of some calculations would be excesive, so it would be necessary to write intermediate tables, that allow us to execute the process taking care of the memory. For this we can use *save parquet*, with the option *partitionBy* to save time when we are interested only in specific parts of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to parquet  \n",
    "CV_data.write\\\n",
    ".mode(\"overwrite\").partitionBy(\"partition_id\").parquet('data/partitioned_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we partitionate the data using the last day of the month to represent an entire moth. In this way we can read and modify specific partitions without to load all the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fig/partition_by_date.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we can load the entire table, that would be inefficient in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "|trans_id|account_id|  date|  type|operation|amount|balance|k_symbol|bank|account| date_time|partition_id|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "| 3626680|      2907|980930|PRIJEM|     null|  85.1|22113.0|    UROK|null|   null|1998-01-30|  1998-01-31|\n",
      "| 3627646|      2936|980930|PRIJEM|     null| 191.3|46995.0|    UROK|null|   null|1998-01-30|  1998-01-31|\n",
      "| 3626619|      2906|980930|PRIJEM|     null|  98.5|20621.5|    UROK|null|   null|1998-01-30|  1998-01-31|\n",
      "| 3627367|      2929|980930|PRIJEM|     null| 335.5|63704.6|    UROK|null|   null|1998-01-30|  1998-01-31|\n",
      "| 3627279|      2927|980930|PRIJEM|     null| 420.0|56245.5|    UROK|null|   null|1998-01-30|  1998-01-31|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_data2 = spark.read.parquet('data/partitioned_data')\n",
    "CV_data2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can load an specific partition to load the data. In this case the colunm *partition_id* is not loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+\n",
      "|trans_id|account_id|  date|  type|operation|amount|balance|k_symbol|bank|account| date_time|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+\n",
      "| 3626680|      2907|980930|PRIJEM|     null|  85.1|22113.0|    UROK|null|   null|1998-01-30|\n",
      "| 3627646|      2936|980930|PRIJEM|     null| 191.3|46995.0|    UROK|null|   null|1998-01-30|\n",
      "| 3626619|      2906|980930|PRIJEM|     null|  98.5|20621.5|    UROK|null|   null|1998-01-30|\n",
      "| 3627367|      2929|980930|PRIJEM|     null| 335.5|63704.6|    UROK|null|   null|1998-01-30|\n",
      "| 3627279|      2927|980930|PRIJEM|     null| 420.0|56245.5|    UROK|null|   null|1998-01-30|\n",
      "+--------+----------+------+------+---------+------+-------+--------+----+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_data_01_98 = spark.read.parquet('data/partitioned_data/' + 'partition_id=1998-01-31')\n",
    "CV_data_01_98.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other recomentations:**\n",
    "\n",
    "* When we do transformations is better to select (using the select command) only those columns necessary for the operations, in order to reduce the size of the table.\n",
    "* In the time to proceeed to joint columns, it is not necessary to use *sort()*, spark can handle the joint operation without this. So, we can save time of processing avoiding the transformation *sort()* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Avoid collect command**\n",
    "\n",
    "To see the data, sometimes is better to save it in parquet, as before you can partition the data, over variables or only the number of the rows. Alternatives to partitionate the data are *partition* or *coalesce*. Here, we partitionated in two and three parts to compare the time spent. The load it is not equal than the case of the command *partitionBy* explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 9.38 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## Inefficient code ----------------------------------------------------------\n",
    "\n",
    "result = CV_data.collect() #will cause driver to collect the results\n",
    "\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 3.31 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "## better code -------------------------------------------------\n",
    "CV_data.repartition(2).write.mode(\"overwrite\").csv(\"data/test_2.csv\") \n",
    "## will assign 2 executors\n",
    "## to collect the result. Assuming executor are better provisioned\n",
    "## -----------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fig/two_partitions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 3.56 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "## better code -------------------------------------------------\n",
    "CV_data.coalesce(3).write.mode(\"overwrite\").csv(\"data/test_3.csv\") \n",
    "## will assign x executors\n",
    "## to collect the result. Assuming executor are better provisioned\n",
    "## -----------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fig/three_partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note**\n",
    "\n",
    "You can use repartition along with selected variables that serves to group the results:\n",
    "\n",
    "```\n",
    "red_mov_with_customer.write\\\n",
    ".repartition(100,_PKEY_C2C_INFO_)\n",
    ".parquet('red_mov')\n",
    "```\n",
    "\n",
    "where _PKEY_C2C_INFO_ is a list of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Avoid additional transformations**\n",
    "Like in the case of *sort*, we have to avoid to use additional operations that delay the results. Thera are functions that spark give us that simplify many processes that we need.\n",
    "For example row_number() over a window can help us to enumerate some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_id</th>\n",
       "      <th>account_id</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>operation</th>\n",
       "      <th>amount</th>\n",
       "      <th>balance</th>\n",
       "      <th>k_symbol</th>\n",
       "      <th>bank</th>\n",
       "      <th>account</th>\n",
       "      <th>date_time</th>\n",
       "      <th>partition_id</th>\n",
       "      <th>row_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3530735</td>\n",
       "      <td>10</td>\n",
       "      <td>970531</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>None</td>\n",
       "      <td>196.4</td>\n",
       "      <td>42470.4</td>\n",
       "      <td>UROK</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2678</td>\n",
       "      <td>10</td>\n",
       "      <td>970608</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>VYBER</td>\n",
       "      <td>23840.0</td>\n",
       "      <td>18615.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-01-08</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2622</td>\n",
       "      <td>10</td>\n",
       "      <td>970612</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>PREVOD NA UCET</td>\n",
       "      <td>7033.0</td>\n",
       "      <td>11582.8</td>\n",
       "      <td>SIPO</td>\n",
       "      <td>UV</td>\n",
       "      <td>18686104.0</td>\n",
       "      <td>1997-01-12</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2586</td>\n",
       "      <td>10</td>\n",
       "      <td>970613</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>26529.0</td>\n",
       "      <td>38111.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-01-13</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2766</td>\n",
       "      <td>10</td>\n",
       "      <td>970624</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>700.0</td>\n",
       "      <td>38811.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-01-24</td>\n",
       "      <td>1997-01-31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056315</th>\n",
       "      <td>3410801</td>\n",
       "      <td>11320</td>\n",
       "      <td>950131</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>VYBER</td>\n",
       "      <td>14.6</td>\n",
       "      <td>30698.8</td>\n",
       "      <td>SLUZBY</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056316</th>\n",
       "      <td>3529384</td>\n",
       "      <td>11320</td>\n",
       "      <td>950131</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>None</td>\n",
       "      <td>150.9</td>\n",
       "      <td>30713.4</td>\n",
       "      <td>UROK</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056317</th>\n",
       "      <td>3410689</td>\n",
       "      <td>11320</td>\n",
       "      <td>950207</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>VKLAD</td>\n",
       "      <td>17279.0</td>\n",
       "      <td>47977.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-01-07</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056318</th>\n",
       "      <td>3410868</td>\n",
       "      <td>11320</td>\n",
       "      <td>950215</td>\n",
       "      <td>VYDAJ</td>\n",
       "      <td>VYBER</td>\n",
       "      <td>10500.0</td>\n",
       "      <td>37477.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-01-15</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056319</th>\n",
       "      <td>3458926</td>\n",
       "      <td>11320</td>\n",
       "      <td>950228</td>\n",
       "      <td>PRIJEM</td>\n",
       "      <td>None</td>\n",
       "      <td>134.3</td>\n",
       "      <td>37686.3</td>\n",
       "      <td>UROK</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995-01-28</td>\n",
       "      <td>1995-01-31</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056320 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         trans_id  account_id    date    type       operation   amount  \\\n",
       "0         3530735          10  970531  PRIJEM            None    196.4   \n",
       "1            2678          10  970608   VYDAJ           VYBER  23840.0   \n",
       "2            2622          10  970612   VYDAJ  PREVOD NA UCET   7033.0   \n",
       "3            2586          10  970613  PRIJEM           VKLAD  26529.0   \n",
       "4            2766          10  970624  PRIJEM           VKLAD    700.0   \n",
       "...           ...         ...     ...     ...             ...      ...   \n",
       "1056315   3410801       11320  950131   VYDAJ           VYBER     14.6   \n",
       "1056316   3529384       11320  950131  PRIJEM            None    150.9   \n",
       "1056317   3410689       11320  950207  PRIJEM           VKLAD  17279.0   \n",
       "1056318   3410868       11320  950215   VYDAJ           VYBER  10500.0   \n",
       "1056319   3458926       11320  950228  PRIJEM            None    134.3   \n",
       "\n",
       "         balance k_symbol  bank     account   date_time partition_id  \\\n",
       "0        42470.4     UROK  None         NaN  1997-01-31   1997-01-31   \n",
       "1        18615.8     None  None         NaN  1997-01-08   1997-01-31   \n",
       "2        11582.8     SIPO    UV  18686104.0  1997-01-12   1997-01-31   \n",
       "3        38111.8     None  None         NaN  1997-01-13   1997-01-31   \n",
       "4        38811.8     None  None         NaN  1997-01-24   1997-01-31   \n",
       "...          ...      ...   ...         ...         ...          ...   \n",
       "1056315  30698.8   SLUZBY  None         NaN  1995-01-31   1995-01-31   \n",
       "1056316  30713.4     UROK  None         NaN  1995-01-31   1995-01-31   \n",
       "1056317  47977.8     None  None         NaN  1995-01-07   1995-01-31   \n",
       "1056318  37477.8     None  None         NaN  1995-01-15   1995-01-31   \n",
       "1056319  37686.3     UROK  None         NaN  1995-01-28   1995-01-31   \n",
       "\n",
       "         row_number  \n",
       "0                 1  \n",
       "1                 2  \n",
       "2                 3  \n",
       "3                 4  \n",
       "4                 5  \n",
       "...             ...  \n",
       "1056315          62  \n",
       "1056316          63  \n",
       "1056317          64  \n",
       "1056318          65  \n",
       "1056319          66  \n",
       "\n",
       "[1056320 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "CV_data2 = CV_data2.withColumn(\"row_number\", F.row_number()\\\n",
    "                    .over(Window.partitionBy(\"account_id\", \"partition_id\")\\\n",
    "                          .orderBy(\"partition_id\")))\n",
    "# we have to avoid pandas, but to see the results in this case is useful to see\n",
    "#all the columns\n",
    "CV_data2.toPandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Persistence**\n",
    "\n",
    "Spark DataFrames can be *saved* or *cached* in Spark memory with the persist(). The persist() commnad allows saving the DataFrame using different storage levels:\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "* MEMORY_ONLY: stores objects in the Spark memory\n",
    "* MEMORY_AND_DISK: stores serialized objects in the Spark memory\n",
    "* DISK_ONLY: stores the data on the local disk\n",
    "--------------------------------------------------------------------------\n",
    "Ex.\n",
    "\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "\n",
    "This is useful when you will use the results of an specific data.frame to get different data.frames, so in order to not repeat the same operations several times you cache or persist the original data.frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For example:**\n",
    "We want to keep only the last transaction of each day for each account, to later apply other transfomation to the data.frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-------+------------+\n",
      "| date_time|trans_id|account_id|balance|partition_id|\n",
      "+----------+--------+----------+-------+------------+\n",
      "|1995-01-05|      58|         1|19035.3|  1995-01-31|\n",
      "|1995-01-13|       5|         1| 4679.0|  1995-01-31|\n",
      "|1995-01-19|     206|         1|19821.1|  1995-01-31|\n",
      "|1995-01-20|     204|         1|22014.3|  1995-01-31|\n",
      "|1995-01-21|     203|         1|21402.7|  1995-01-31|\n",
      "+----------+--------+----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_data_0 = spark.read.parquet('data/partitioned_data/')\n",
    "CV_transf = CV_data_0.select(['date_time', 'trans_id', 'account_id', 'balance', 'partition_id'])\\\n",
    ".sort(\"account_id\", \"date_time\", \"trans_id\")\\\n",
    ".drop_duplicates(subset=['date_time', 'account_id'])\\\n",
    ".sort(\"account_id\", \"date_time\", \"trans_id\")\n",
    "\n",
    "CV_transf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [account_id#404 ASC NULLS FIRST, date_time#413 ASC NULLS FIRST, trans_id#403 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(account_id#404 ASC NULLS FIRST, date_time#413 ASC NULLS FIRST, trans_id#403 ASC NULLS FIRST, 200)\n",
      "   +- *(3) HashAggregate(keys=[date_time#413, account_id#404], functions=[first(trans_id#403, false), first(balance#409, false), first(partition_id#414, false)])\n",
      "      +- Exchange hashpartitioning(date_time#413, account_id#404, 200)\n",
      "         +- *(2) HashAggregate(keys=[date_time#413, account_id#404], functions=[partial_first(trans_id#403, false), partial_first(balance#409, false), partial_first(partition_id#414, false)])\n",
      "            +- *(2) Sort [account_id#404 ASC NULLS FIRST, date_time#413 ASC NULLS FIRST, trans_id#403 ASC NULLS FIRST], true, 0\n",
      "               +- Exchange rangepartitioning(account_id#404 ASC NULLS FIRST, date_time#413 ASC NULLS FIRST, trans_id#403 ASC NULLS FIRST, 200)\n",
      "                  +- *(1) Project [date_time#413, trans_id#403, account_id#404, balance#409, partition_id#414]\n",
      "                     +- *(1) FileScan parquet [trans_id#403,account_id#404,balance#409,date_time#413,partition_id#414] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/erikapat/Dropbox/PRUEBAS_DATA_SCIENCE/SPARK/GIT_SPARK-PRACTICE-NOTE..., PartitionCount: 6, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trans_id:int,account_id:int,balance:double,date_time:date>\n"
     ]
    }
   ],
   "source": [
    "CV_transf.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the result to a (1) database and later (2) apply some additional transformation, then each time you would repeat the operations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ineficient code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 20.31 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## Inefficient code (1)----------------------------------------------------------\n",
    "CV_transf.write\\\n",
    ".mode(\"overwrite\").partitionBy(\"partition_id\").parquet('data/partitioned_data_2')\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.02 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## Inefficient code (2)----------------------------------------------------------\n",
    "CV_transf_2 = CV_transf\\\n",
    ".groupby(['trans_id', 'account_id', 'balance', 'partition_id'])\\\n",
    ".avg('balance')\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Efficient code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_transf = CV_transf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 20.48 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## Efficient code (1)----------------------------------------------------------\n",
    "CV_transf.write\\\n",
    ".mode(\"overwrite\").partitionBy(\"partition_id\")\\\n",
    ".parquet('data/partitioned_data_2')\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.01 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## Efficient code (2)----------------------------------------------------------\n",
    "CV_transf_2 = CV_transf\\\n",
    ".groupby(['trans_id', 'account_id', 'balance', 'partition_id'])\\\n",
    ".avg('balance')\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date_time: date, trans_id: int, account_id: int, balance: double, partition_id: date]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## do not forget\n",
    "CV_transf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plan of execution**\n",
    "\n",
    "There is not difference between the order of filter or select looking the plan of execution of spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "CV = spark.read.parquet('data/partitioned_data/')\n",
    "df1 = CV.select('trans_id', 'account_id', 'amount', 'balance', 'partition_id')\\\n",
    ".filter(f.col('account_id')== '2907')\n",
    "df2 = CV.filter(f.col('account_id')== '2907')\\\n",
    ".select('trans_id', 'account_id', 'amount', 'balance', 'partition_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [trans_id#628, account_id#629, amount#633, balance#634, partition_id#639]\n",
      "+- *(1) Filter (isnotnull(account_id#629) && (account_id#629 = 2907))\n",
      "   +- *(1) FileScan parquet [trans_id#628,account_id#629,amount#633,balance#634,partition_id#639] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/erikapat/Dropbox/PRUEBAS_DATA_SCIENCE/SPARK/GIT_SPARK-PRACTICE-NOTE..., PartitionCount: 6, PartitionFilters: [], PushedFilters: [IsNotNull(account_id), EqualTo(account_id,2907)], ReadSchema: struct<trans_id:int,account_id:int,amount:double,balance:double>\n"
     ]
    }
   ],
   "source": [
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [trans_id#628, account_id#629, amount#633, balance#634, partition_id#639]\n",
      "+- *(1) Filter (isnotnull(account_id#629) && (account_id#629 = 2907))\n",
      "   +- *(1) FileScan parquet [trans_id#628,account_id#629,amount#633,balance#634,partition_id#639] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/erikapat/Dropbox/PRUEBAS_DATA_SCIENCE/SPARK/GIT_SPARK-PRACTICE-NOTE..., PartitionCount: 6, PartitionFilters: [], PushedFilters: [IsNotNull(account_id), EqualTo(account_id,2907)], ReadSchema: struct<trans_id:int,account_id:int,amount:double,balance:double>\n"
     ]
    }
   ],
   "source": [
    "df2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-------+------------+\n",
      "|trans_id|account_id|amount|balance|partition_id|\n",
      "+--------+----------+------+-------+------------+\n",
      "| 3626680|      2907|  85.1|22113.0|  1998-01-31|\n",
      "|  853005|      2907|6891.0|28989.4|  1998-01-31|\n",
      "|  853077|      2907|3983.0|25006.4|  1998-01-31|\n",
      "|  853233|      2907|  14.6|25089.4|  1998-01-31|\n",
      "| 3626681|      2907|  97.6|25104.0|  1998-01-31|\n",
      "+--------+----------+------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time require to run the model: 0.13 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "CV.select('trans_id', 'account_id', 'amount', 'balance', 'partition_id')\\\n",
    ".filter(f.col('account_id')== '2907').show(5)\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-------+------------+\n",
      "|trans_id|account_id|amount|balance|partition_id|\n",
      "+--------+----------+------+-------+------------+\n",
      "| 3626680|      2907|  85.1|22113.0|  1998-01-31|\n",
      "|  853005|      2907|6891.0|28989.4|  1998-01-31|\n",
      "|  853077|      2907|3983.0|25006.4|  1998-01-31|\n",
      "|  853233|      2907|  14.6|25089.4|  1998-01-31|\n",
      "| 3626681|      2907|  97.6|25104.0|  1998-01-31|\n",
      "+--------+----------+------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Time require to run the model: 0.08 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "CV.filter(f.col('account_id')== '2907')\\\n",
    ".select('trans_id', 'account_id', 'amount', 'balance', 'partition_id').show(5)\n",
    "## -----------------------------------------------------------------------------\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **join broadcast**\n",
    "\n",
    "Spark broadcast joins are perfect for joining a large DataFrame with a small DataFrame. Broadcast joins cannot be used when joining two large DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "modelDataFile = \"../data/bank-transactions/account.asc\"\n",
    "\n",
    "client_data = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .option(\"delimiter\", \";\") \\\n",
    "  .load(modelDataFile)\n",
    "\n",
    "client_data = client_data.select('account_id', 'district_id')\\\n",
    ".drop_duplicates(subset=['account_id', 'district_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = spark.read.parquet('data/partitioned_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056320\n",
      "Time require to run the model: 0.23 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "#You do the broadcast in the small dataset\n",
    "data_joined = CV.join(broadcast(client_data), \n",
    "                 CV.account_id == client_data.account_id)\n",
    "print(data_joined.count())\n",
    "## --------------------------------------------------------------\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056320\n",
      "Time require to run the model: 0.22 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution \n",
    "timestart= datetime.datetime.now()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "# Perform join\n",
    "data_joined = client_data.join(CV, client_data.account_id == CV.account_id)\n",
    "print(data_joined.count())\n",
    "## --------------------------------------------------------------\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* WHY YOUR SPARK APPS ARE SLOW OR FAILING: PART I MEMORY MANAGEMENT. [Here]()\n",
    "* WHY YOUR SPARK APPS ARE SLOW OR FAILING: PART II DATA SKEW AND GARBAGE COLLECTION [Here](https://unraveldata.com/common-failures-slowdowns-part-ii/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
