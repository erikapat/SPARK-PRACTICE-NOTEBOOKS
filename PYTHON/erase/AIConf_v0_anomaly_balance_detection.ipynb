{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo notebook: anomaly detection\n",
    "\n",
    "This is a demo code to show the functionality of the balance anomaly detection engine in the Relevant Facts project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pnd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [15,7]\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/opt/cloudera/parcels/SPARK2/lib/spark2/python')\n",
    "#sys.path.insert(0, '.')\n",
    "sys.path.insert(0, '/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip')\n",
    "sys.path.insert(0, '/us/e055324/hechos_relevantes/rf_16_demo/da_srm_relevant_facts')\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/DYA/da_tech/envs/advice_pfm/bin/python'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/DYA/da_tech/envs/advice_pfm/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "pw = getpass.getpass()\n",
    "!kinit <<< $pw \n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "CONF = (pyspark.SparkConf()\n",
    "            .setMaster(\"yarn\")\n",
    "            .setAppName(\"AIConf_demo_HR\"))\n",
    "\n",
    "CONF = CONF.set(\"spark.executor.memory\", \"1g\") \\\n",
    "    .set(\"spark.driver.memory\", \"1g\") \\\n",
    "    .set(\"spark.dynamicAllocation.enabled\", \"True\") \\\n",
    "    .set(\"spark.executor.cores\", \"2\") \\\n",
    "    .set(\"spark.yarn.queue\", \"root.DyA.DS\") \\\n",
    "    .set(\"spark.executor.memoryOverhead\", \"1024\") \\\n",
    "    .set(\"spark.driver.memoryOverhead\", \"1024\")\\\n",
    "    .set(\"spark.port.maxRetries\", 100) \\\n",
    "    .set('spark.dynamicAllocation.maxExecutors', 5) \\\n",
    "    .set('spark.dynamicAllocation.minExecutors', 3) \\\n",
    "    .set('spark.sql.sources.partitionOverwriteMode', \"dynamic\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=CONF).getOrCreate()\n",
    "hiveContext = HiveContext(spark)\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal dependencies and configuration\n",
    "\n",
    "The code in this demo uses the pre-productive version of Relevant Fact 16 (RF16) and the productive model for balance forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyhocon import ConfigFactory\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import hr.datasources.transactions as transactions\n",
    "from hr.datasources.operations.transactions_sql_operations import transactions_between_interval\n",
    "from hr.datasources.operations.transactions_sql_operations import filter_titularity, filter_visible_product\n",
    "from hr.datasources.operations.transactions_sql_operations import filter_commercial_communications\n",
    "from pyspark.sql.functions import col, abs, size, udf, lit\n",
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from hr.daily.rf16_balance_anomaly.rf16_balance_anomaly import RF16\n",
    "import hr.datasources.clients as clients\n",
    "from hr.datasources.operations.transactions_sql_operations import join_clients_with_transactions\n",
    "from hr.datasources.ts_balanced import TsBalanced\n",
    "#from hr.forecasting_balance.forecasting_transform import ForecastingBalanceTransform\n",
    "\n",
    "spark.sparkContext.addPyFile(\"/us/e055324/hechos_relevantes/rf_16_demo/da_srm_relevant_facts_dist/hr_dist.zip\")\n",
    "#spark.sparkContext.addPyFile(\"/us/e043894/relevant_facts/release_0_0_10/hr_dist.zip\")\n",
    "#path_model = \"/us/e055324/hechos_relevantes/develop/da_srm_relevant_facts/hr/model/11q_model\"\n",
    "\n",
    "CUSTOM_DATA_DEPENDENCIES = \"CUSTOM_DATA_DEPENDENCIES\"\n",
    "custom_data_config = {}\n",
    "custom_data_config.update({CUSTOM_DATA_DEPENDENCIES:\n",
    "                           ConfigFactory.parse_file(\"/us/e055324/hechos_relevantes/rf_16_demo/da_srm_relevant_facts/hr/datasources/conf/datasources_dependencies_dev.json\")})\n",
    "\n",
    "day = datetime.strptime(\"20191003\", \"%Y%m%d\")\n",
    "yesterday = day - relativedelta(days=1)\n",
    "start_day=day - relativedelta(months=6)\n",
    "end_day=day - relativedelta(days=1)\n",
    "norm_tolerance = 0.01\n",
    "k_low = 4\n",
    "k_high = 4\n",
    "thr_variation_ratio_main_mov = 0.9\n",
    "lag = 0\n",
    "win_l = 1\n",
    "        \n",
    "categorizer_output_path = (custom_data_config[CUSTOM_DATA_DEPENDENCIES][\"transactions\"][\"movements_categoriser_output\"][\"hdfs_path\"].format(env=\"dev\", release_type=\"release_candidate\"))\n",
    "execution_plan={'RF16': [datetime(2019, 10, 3, 0, 0)]}\n",
    "\n",
    "dataset_transactions_opath = \"da_hechos_dev.tc_dataset_demo\"\n",
    "dataset_ts_balance_opath = \"da_hechos_dev.ts_dataset_demo\"\n",
    "dataset_balance_pred_opath = \"da_hechos_dev.bp_dataset_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo dataset\n",
    "\n",
    "We have defined a reduced dataset that comprises some illustrative series. This chunk filters the corresponding transactions, composes time series and obtains balance forecastings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transactions_clients_df = spark.read.table(dataset_transactions_opath).persist()\n",
    "ts_balanced = spark.read.table(dataset_ts_balance_opath).persist()\n",
    "balance_with_prediction = spark.read.table(dataset_balance_pred_opath).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions to analyze and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "def plot_anomalous_series (cod_idcontra, df_results):\n",
    "    \n",
    "    n_res = df_results.filter(sf.col('cod_idcontra')==cod_idcontra).count()\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [15,5]\n",
    "    \n",
    "    if (n_res == 0):\n",
    "        print('No series!')\n",
    "        return\n",
    "    \n",
    "    pnd_sample_ts = (df_results\n",
    "                     .filter(sf.col('cod_idcontra')==cod_idcontra)\n",
    "                     .select(['time_series','predict','thr_low', 'thr_high'])\n",
    "                     .toPandas()\n",
    "                    )\n",
    "    \n",
    "    \n",
    "\n",
    "    ax1.plot(range(len(pnd_sample_ts.time_series[0])),\n",
    "                     pnd_sample_ts.time_series[0],'-o')\n",
    "\n",
    "\n",
    "    ax1.plot(range(len(pnd_sample_ts.time_series[0][0:-1]+pnd_sample_ts.predict[0][5])),\n",
    "                     pnd_sample_ts.time_series[0][0:-1]+pnd_sample_ts.predict[0][5],'--k')\n",
    "\n",
    "    ax1.legend(['Real (current)','Real (prev) + Median'])\n",
    "    ax1.set_ylabel('Balance (€)')\n",
    "    ax1.set_xlabel('Day')\n",
    "\n",
    "    ax1.axhspan(pnd_sample_ts.thr_low[0][0], pnd_sample_ts.thr_high[0][0],color='green',alpha=.1)\n",
    "    \n",
    "    for i in range(5):\n",
    "        ax1.fill_between(range(len(pnd_sample_ts.time_series[0][0:-1]+pnd_sample_ts.predict[0][i])),\n",
    "                         pnd_sample_ts.time_series[0][0:-1]+pnd_sample_ts.predict[0][i],\n",
    "                         pnd_sample_ts.time_series[0][0:-1]+pnd_sample_ts.predict[0][10-i],\n",
    "                        alpha=.1,\n",
    "                        color='blue')\n",
    "        \n",
    "    \n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Anomaly detection in series\n",
    "\n",
    "We use a threshold-based anomaly detection approach that relies on the quantile regression results. \n",
    "Specifically, our engine raises an alarm if\n",
    "\n",
    "\\begin{align}\n",
    "(\\mathbb{P}\\{Balance(t) < X_t\\} < Thr_{min}) \\lor (\\mathbb{P}\\{Balance(t) > X_t\\} < Thr_{max})\n",
    "\\end{align}\n",
    "\n",
    "where $Balance(t)$ is the predicted quantiles for time $t$, $X_t$ is the series value at time $t$, and $(Thr_{min}+ Thr_{max})$ is the anomalous range's probability. To offer noticeable results without overwhelming clients, we heuristically control $(Thr_{min}+ Thr_{max})$ by considering as anomalies those observations outside an interval of the form:\n",
    "\n",
    "\\begin{align}\n",
    "[Q_5 − k_1\\frac{(Q_{50}-Q_5)}{(Q_{95}-Q_5)}(Q_{95}-Q_5) , Q_{95} + k_2(1-\\frac{(Q_{50}-Q_5)}{(Q_{95}-Q_5)})(Q_{95}-Q_5)]\n",
    "\\end{align}\n",
    "\n",
    "which is inspired in the proposal by Hubert & Vandervieren (2008) to adapt the well-known Tukey criterion to asymmetric distributions.\n",
    "\n",
    "Here, both $k_1$ and $k_2$ allow introducing differential weights for abnormal values above / below the non-atypical values' range. Note that, as Hinkley's coefficient \n",
    "\n",
    "\\begin{align}\n",
    "0 = \\frac{((Q_{95}-Q_{50}) - (Q_{50}-Q_{5}))}{(Q_{95}-Q_{5})}\n",
    "\\end{align}\n",
    "\n",
    "is 0 for symmetric distributions,\n",
    "if we scale $ [Q_5, Q_{95}] $ to $[0,1]$, then \n",
    "\n",
    "\\begin{align}\n",
    "|Q_{50} - Q_5|+|Q_{95} - Q_{50}| = 1\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align}\n",
    "|Q_{50} - Q_5| \\rightarrow 0.5, |Q_{95} - Q_{50}| \\rightarrow 0.5\n",
    "\\end{align}\n",
    "\n",
    "when the distribution tends to be symmetric.\n",
    "\n",
    "With $k_1 = k_2 = 4$, we have that for fairly symmetric distributions:\n",
    "\n",
    "\\begin{align}\n",
    "[Q_5 − 2(Q_{95}-Q_5) , Q_{95} + 2(Q_{95}-Q_5)]\n",
    "\\end{align}\n",
    "\n",
    "which empirically showed good results in terms of detected events (it may be somehow restrictive in case of Gaussian distributions, but suits the heavier-tailed nature of balance series).\n",
    "Anyhow, these parameters can be tuned to reflect a less restrictive behavior, or to introduce some bias towards the detection of negative or positive divergences.\n",
    "\n",
    "<cite>Hubert, M., & Vandervieren, E. (2008). An adjusted boxplot for skewed distributions. Computational statistics & data analysis, 52(12), 5186-5201.</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_graf(k1=[4,0,1,10],\n",
    "              k2=[4,0,1,10],\n",
    "             cod_idcontra = ['00000000000000000000XXXXXX',\n",
    "                             '00000000000000000000XXXXXX',\n",
    "                             '00000000000000000000XXXXXX']):\n",
    "    \n",
    "    \n",
    "    rf16_transform = RF16(day=day)\n",
    "    \n",
    "    rf16_transform.k_high = k2\n",
    "    rf16_transform.k_low = k1\n",
    "    \n",
    "    df_anomalies = rf16_transform.obtain_rfs_tosend(transactions_clients_df, \n",
    "                                                ts_balanced, \n",
    "                                                balance_with_prediction.drop('time_series'))\n",
    "    \n",
    "    plot_anomalous_series (cod_idcontra, df_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
