{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spark tricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **See all the columns of a large dataset**\n",
    "\n",
    "Configuration instructions:\n",
    "\n",
    "In ~/.jupyter/custom/custom.js (to avoid wrap - horizontal scroll):\n",
    "\n",
    "`\n",
    "$([IPython.events]).on('app_initialized.NotebookApp', function(){\n",
    "  IPython.CodeCell.options_default['cm_config']['lineWrapping'] = true;\n",
    "});\n",
    "`\n",
    "And added this in ~/.jupyter/custom/custom.css (to use all width):\n",
    "\n",
    "`\n",
    ".container { width:100% !important; }\n",
    "pre, code, kbd, samp {\n",
    "    white-space: pre;\n",
    "}\n",
    "`\n",
    "\n",
    "These two tricks help to make the Spark SQL DataFrame show() method a little more \"palatable\" by aligning columns to effectively kill word wrap and giving more width to the view. If these files do not exist, try creating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'PassengerId2': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name2': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex2': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived2': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0},\n",
    "         'Age2': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare2': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass2': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'Pclass3': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'Pclass4': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}\n",
    "            }\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "data1 = spark.createDataFrame(df1_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|PassengerId2|   Name2|  Sex2|Survived2|Age2|Fare2|Pclass2|Pclass3|Pclass4|\n",
      "+-----------+--------+------+--------+---+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|           1|    Owen|  male|        0|  22|  7.3|      3|      3|      3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|           2|Florence|female|        1|  38| 71.3|      1|      1|      1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|           3|   Laina|female|        1|  26|  7.9|      3|      3|      3|\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|           4|    Lily|female|        1|  35| 53.1|      1|      1|      1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|           5| William|  male|        0|  35|  8.0|      3|      3|      3|\n",
      "+-----------+--------+------+--------+---+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use partitions**\n",
    "\n",
    "By default, when we perform a shuffle Spark will output two hundred shuffle partitions. We will set this value from 1 to five in order to reduce the number of the output partitions from the shuffle from two hundred to five.\n",
    "\n",
    "Go ahead and experiment with different values and see the number of partitions yourself. In experimenting with different values, you should see drastically different run times. Remenber that you can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.18 segundos\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# load data\n",
    "\n",
    "flightData2015 = spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"../data/2015-summary.csv\") \n",
    "\n",
    "\n",
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.08 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic dataframe terms**\n",
    "We define that we think are five basic verbs â€” select, filter, mutate, summarize, and arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| Owe|\n",
      "| Flo|\n",
      "| Lai|\n",
      "| Lil|\n",
      "| Wil|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(df1.Name.substr(1, 3).alias(\"name\")) .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+\n",
      "|PassengerId|   Sex|startswith(Name, Ow)|\n",
      "+-----------+------+--------------------+\n",
      "|          1|  male|                true|\n",
      "|          2|female|               false|\n",
      "|          3|female|               false|\n",
      "|          4|female|               false|\n",
      "|          5|  male|               false|\n",
      "+-----------+------+--------------------+\n",
      "\n",
      "+-----------+------+------------------+\n",
      "|PassengerId|   Sex|endswith(Name, am)|\n",
      "+-----------+------+------------------+\n",
      "|          1|  male|             false|\n",
      "|          2|female|             false|\n",
      "|          3|female|             false|\n",
      "|          4|female|             false|\n",
      "|          5|  male|              true|\n",
      "+-----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"PassengerId\", \"Sex\", df1.Name.startswith(\"Ow\")).show(5)\n",
    "df1.select(\"PassengerId\", \"Sex\", df1.Name.endswith(\"am\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one way\n",
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+--------+\n",
      "|PassengerId|   Name|   Sex|Survived|\n",
      "+-----------+-------+------+--------+\n",
      "|          3|  Laina|female|       1|\n",
      "|          4|   Lily|female|       1|\n",
      "|          5|William|  male|       0|\n",
      "+-----------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1[\"PassengerId\"]>2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second way: sql expression\n",
    "df1.filter(\"Sex='female'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## other way to filter strings\n",
    "df1.filter(df1.Sex.contains(\"female\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sql like statement, you use % to indicate that there are more...\n",
    "df1.filter(df1.Sex.like(\"fem%\")).show()\n",
    "# or\n",
    "#df1.filter(df1.Sex.like(\"%em%\")).show()\n",
    "# or \n",
    "#df1.filter(df1.Sex.like(\"%emale\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sql statement similar to like, it is not necessary to put '%'\n",
    "df1.filter(df1.Sex.rlike(\"fem\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+--------+\n",
      "|PassengerId|   Name|   Sex|Survived|\n",
      "+-----------+-------+------+--------+\n",
      "|          1|   Owen|  male|       0|\n",
      "|          4|   Lily|female|       1|\n",
      "|          5|William|  male|       0|\n",
      "+-----------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.filter(~col('PassengerId').isin(['2','3'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+--------+\n",
      "|PassengerId| Name|   Sex|Survived|\n",
      "+-----------+-----+------+--------+\n",
      "|          3|Laina|female|       1|\n",
      "|          4| Lily|female|       1|\n",
      "+-----------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.PassengerId.between(3, 4)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Where**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------+--------+\n",
      "|PassengerId| Name|   Sex|Survived|\n",
      "+-----------+-----+------+--------+\n",
      "|          3|Laina|female|       1|\n",
      "|          4| Lily|female|       1|\n",
      "+-----------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(df1.PassengerId.between(3, 4)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+--------+\n",
      "|PassengerId|   Name|   Sex|Survived|\n",
      "+-----------+-------+------+--------+\n",
      "|          1|   Owen|  male|       0|\n",
      "|          4|   Lily|female|       1|\n",
      "|          5|William|  male|       0|\n",
      "+-----------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(~col('PassengerId').isin(['2','3'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mutate**: creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+--------------------+\n",
      "|PassengerId|Age|Fare|Pclass|          new_column|\n",
      "+-----------+---+----+------+--------------------+\n",
      "|          1| 22| 7.3|     3|This is a new column|\n",
      "|          2| 38|71.3|     1|This is a new column|\n",
      "|          3| 26| 7.9|     3|This is a new column|\n",
      "|          4| 35|53.1|     1|This is a new column|\n",
      "|          5| 35| 8.0|     3|This is a new column|\n",
      "+-----------+---+----+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df2.withColumn('new_column', F.lit('This is a new column')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          1| 22| 7.3|     3|\n",
      "|          2| 38|71.3|     1|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.drop(\"new_column\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Na's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: bigint, Age: bigint, Fare: double, Pclass: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.na.drop()\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+\n",
      "|PassengerId|    Name|   Sex|Survived|set|\n",
      "+-----------+--------+------+--------+---+\n",
      "|          1|    Owen|  male|       0|  0|\n",
      "|          2|Florence|female|       1|  0|\n",
      "|          3|   Laina|female|       1|  0|\n",
      "|          4|    Lily|female|       1|  0|\n",
      "|          5| William|  male|       0|  1|\n",
      "+-----------+--------+------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "df1.withColumn(\"set\", F.when( df1.PassengerId > 4, 1 ).otherwise( 0 )).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+\n",
      "|PassengerId|    Name|   Sex|Survived|set|\n",
      "+-----------+--------+------+--------+---+\n",
      "|          1|    Owen|  male|       0|  0|\n",
      "|          2|Florence|female|       1|  0|\n",
      "|          3|   Laina|female|       1|  0|\n",
      "|          4|    Lily|female|       1|  0|\n",
      "|          5| William|  male|       0|  1|\n",
      "+-----------+--------+------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf \n",
    "df1.withColumn(\"set\", sf.when( df1.PassengerId > 4, 1 ).otherwise( 0 )).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+\n",
      "|PassengerId|    Name|   Sex|Survived|set|\n",
      "+-----------+--------+------+--------+---+\n",
      "|          1|    Owen|  male|       0|  0|\n",
      "|          2|Florence|female|       1|  0|\n",
      "|          3|   Laina|female|       1|  0|\n",
      "|          4|    Lily|female|       1|  0|\n",
      "|          5| William|  male|       0|  1|\n",
      "+-----------+--------+------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f \n",
    "df1.withColumn(\"set\", f.when( df1.PassengerId > 4, 1 ).otherwise( 0 )).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summarize** using group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2 = df2.groupby('Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf2.count().select(\n",
    "#  'count'\n",
    "#).rdd.flatMap(\n",
    "#  lambda x: x\n",
    "#).histogram(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "|     1|              36.5|             62.2|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call multiple aggregation functions at once, pass a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "|     1|       2|              36.5|    124.4|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The toDF() method can be called on a sequence object to create a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "|     1|     2|              36.5|     124.4|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\\\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Count Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          1| 22| 7.3|     3|\n",
      "|          2| 38|71.3|     1|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          5|  4|   5|     2|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "df2.agg(*(countDistinct(col(c)).alias(c) for c in df2.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---+----+------+\n",
      "|Pclass|PassengerId|Age|Fare|Pclass|\n",
      "+------+-----------+---+----+------+\n",
      "|     3|          3|  3|   3|     1|\n",
      "|     1|          2|  2|   2|     1|\n",
      "+------+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "df2.groupby('Pclass').agg(*(countDistinct(col(c)).alias(c) for c in df2.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          1| 22| 7.3|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          4| 35|53.1|     1|\n",
      "|          2| 38|71.3|     1|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.orderBy('Fare').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Joins and unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join\n",
    "df1.join(df2, ['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unions\n",
    "#Union() returns a dataframe from the union of two dataframes\n",
    "df1.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common symptom of performance issues caused by chained unions in a for loop is it took longer and longer to iterate through the loop. In this case, **repartition()** and **checkpoint()** may help solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The spark.sql API**\n",
    "\n",
    "Many of the operations that I showed can be accessed by writing SQL (Hive) queries in spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "#df.registerTempTable(\"connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other way to do the same\n",
    "\n",
    "```\n",
    "spark.read.parquet('hdfs://sces3p100.hdfsinternalana/user/red_mov_with_customer_2')\n",
    "df = spark.read.format('parquet').load('prueba').registerTempTable(\"tmp\")\n",
    "spark.sql('''select * from tmp''').show(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create empty data.frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "field = [StructField('cod_pais_1',StringType(), True),\n",
    "        StructField('cod_entidad_1',IntegerType(), True),\n",
    "         StructField('cod_id_1',IntegerType(), True),\n",
    "         StructField('cod_persona_1',IntegerType(), True),\n",
    "         StructField('fec_movim_1',IntegerType(), True),\n",
    "         StructField('fec_month_1',IntegerType(), True),\n",
    "         StructField('partition_1',IntegerType(), True)\n",
    "        ]\n",
    "schema = StructType(field)\n",
    "#schema = cl_contratos_nomina.printSchema()\n",
    "table_name = sqlContext.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cod_pais_1: string (nullable = true)\n",
      " |-- cod_entidad_1: integer (nullable = true)\n",
      " |-- cod_id_1: integer (nullable = true)\n",
      " |-- cod_persona_1: integer (nullable = true)\n",
      " |-- fec_movim_1: integer (nullable = true)\n",
      " |-- fec_month_1: integer (nullable = true)\n",
      " |-- partition_1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Change the names of the columns in a data.frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+-------------+-----------+-----------+-----------+\n",
      "|cod_pais_1|cod_entidad_1|cod_id_1|cod_persona_1|fec_movim_1|fec_month_1|partition_1|\n",
      "+----------+-------------+--------+-------------+-----------+-----------+-----------+\n",
      "+----------+-------------+--------+-------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_name_list= list(map(lambda x: x.replace(\"_1\", \"\"), table_name.columns))\n",
    "table_name_renamed = table_name.toDF(*new_column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "|cod_pais|cod_entidad|cod_id|cod_persona|fec_movim|fec_month|partition|\n",
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([(1,2), (3,4)], ['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| x1| x2|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data\n",
    "   .withColumnRenamed('x1','x3')\n",
    "   .withColumnRenamed('x2', 'x4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| x3| x4|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to aggregate a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|xx3|xx4|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f \n",
    "data.select([f.col(c).alias('x' + c) for c in col]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to aggregate a suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|x3_x|x4_x|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "(data\n",
    " .select(*[F.col(c).alias(f\"{c}_x\") for c in data.columns])\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   1|  2|\n",
      "|   3|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.selectExpr(\"x3 as name\", \"x4 as age\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fill na**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   1|  2|\n",
      "|   3|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   1|  2|\n",
      "|   3|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill na's for specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: bigint, age: bigint]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(0, subset=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **schema with arrays**\n",
    "\n",
    "**Fisrt way**\n",
    "\n",
    "Sometime one of the columns is an array and we have to define those columns as arrays type.\n",
    "\n",
    "* 1st. Define shema before to load.\n",
    "* 2nd. Define columns that are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- imp_sdopost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_c0\",  StringType(), True),\n",
    "     StructField(\"imp_sdopost\",  StringType(), True)\n",
    "])\n",
    "\n",
    "trends = spark\\\n",
    ".read\\\n",
    ".schema(schema)\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"data/trends.csv\") \n",
    "\n",
    "trends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends= trends.withColumn(\"imp_sdopost\", split(col(\"imp_sdopost\"), \",\").cast(\"array<long>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- imp_sdopost: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|         imp_sdopost|\n",
      "+---+--------------------+\n",
      "|  0|[, 2915, 2912, 28...|\n",
      "|  1|[, 228, 228, 228,...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate in elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|_c0| col|\n",
      "+---+----+\n",
      "|  0|null|\n",
      "|  0|2915|\n",
      "|  0|2912|\n",
      "|  0|2853|\n",
      "|  0|2853|\n",
      "|  0|2853|\n",
      "|  0|2853|\n",
      "|  0|2796|\n",
      "|  0|2796|\n",
      "|  0|2796|\n",
      "|  0|2796|\n",
      "|  0|2431|\n",
      "|  0|2431|\n",
      "|  0|2431|\n",
      "|  0|2339|\n",
      "|  0|2339|\n",
      "|  0|2339|\n",
      "|  0|2339|\n",
      "|  0|2339|\n",
      "|  0|2339|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "trends.select(\"_c0\",explode(\"imp_sdopost\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "|Group|Date|row_num|\n",
      "+-----+----+-------+\n",
      "|    B|1999|      1|\n",
      "|    B|2015|      2|\n",
      "|    A|2000|      1|\n",
      "|    A|2002|      2|\n",
      "|    A|2007|      3|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"A\", 2000), (\"A\", 2002), (\"A\", 2007), (\"B\", 1999), (\"B\", 2015)], [\"Group\", \"Date\"])\n",
    "# accepted solution above\n",
    "\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df = df.withColumn(\"row_num\", row_number().over(Window.partitionBy(\"Group\").orderBy(\"Date\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save CSV**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#worst way\n",
    "flightData2015.write.format(\"csv\"). \\\n",
    "mode(\"overwrite\").options(header=\"true\",sep=\",\").save(path=\"data/data_csv\") #path to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other ways to do it\n",
    "# with repartition\n",
    "flightData2015.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    "   .mode(\"overwrite\").save(\"data/data_csv_2\") #path to folder\n",
    "\n",
    "# with coalesce\n",
    "flightData2015.coalesce(1).write.format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\").save(\"data/data_csv_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better and simplest way\n",
    "flightData2015.repartition(1).write.csv(\"data/test_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save and read Parquet format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.write.partitionBy(\"DEST_COUNTRY_NAME\").format(\"parquet\").save(\"data/flightData2015.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----+-----------------+\n",
      "|            Romania|   15|    United States|\n",
      "|            Croatia|    1|    United States|\n",
      "|            Ireland|  344|    United States|\n",
      "|              India|   62|    United States|\n",
      "|          Singapore|    1|    United States|\n",
      "+-------------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/flightData2015.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tuning performance or debugging dataframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cache a dataframe when it is used multiple times in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: bigint, Name: string, Sex: string, Survived: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we indicate that sometimes chaining too many union() cause performance problem or even out of memory errors. checkpoint() truncates the execution plan and saves the checkpointed dataframe to a temporary \n",
    "location on the disk.\n",
    "\n",
    "2.1. It is recomended caching before checkpointing, so Spark doesnâ€™t have to read in the dataframe from disk after itâ€™s checkpointed.\n",
    "\n",
    "2.2. To use checkpoint(), I need to specify the temporary file location to save the datafame to by accessing the sparkContext object from SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "#sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"checkpointdir\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(8) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#334, Sex#335, Survived#336L, Name#345, Sex#346, Survived#347L]\n",
      "+- *(8) SortMergeJoin [PassengerId#22L], [PassengerId#344L], Inner\n",
      "   :- *(5) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#334, Sex#335, Survived#336L]\n",
      "   :  +- *(5) SortMergeJoin [PassengerId#22L], [PassengerId#333L], Inner\n",
      "   :     :- *(2) Sort [PassengerId#22L ASC NULLS FIRST], false, 0\n",
      "   :     :  +- Exchange hashpartitioning(PassengerId#22L, 5)\n",
      "   :     :     +- *(1) Filter isnotnull(PassengerId#22L)\n",
      "   :     :        +- Scan ExistingRDD[PassengerId#22L,Name#23,Sex#24,Survived#25L]\n",
      "   :     +- *(4) Sort [PassengerId#333L ASC NULLS FIRST], false, 0\n",
      "   :        +- ReusedExchange [PassengerId#333L, Name#334, Sex#335, Survived#336L], Exchange hashpartitioning(PassengerId#22L, 5)\n",
      "   +- *(7) Sort [PassengerId#344L ASC NULLS FIRST], false, 0\n",
      "      +- ReusedExchange [PassengerId#344L, Name#345, Sex#346, Survived#347L], Exchange hashpartitioning(PassengerId#22L, 5)\n"
     ]
    }
   ],
   "source": [
    "#For example, I can join df1 to itself 3 times:\n",
    "df = df1.join(df1, ['PassengerId'])\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#359, Sex#360, Survived#361L, Name#377, Sex#378, Survived#379L]\n",
      "+- *(4) SortMergeJoin [PassengerId#22L], [PassengerId#376L], Inner\n",
      "   :- *(1) Filter isnotnull(PassengerId#22L)\n",
      "   :  +- Scan ExistingRDD[PassengerId#22L,Name#23,Sex#24,Survived#25L,Name#359,Sex#360,Survived#361L]\n",
      "   +- *(3) Sort [PassengerId#376L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#376L, 5)\n",
      "         +- *(2) Filter isnotnull(PassengerId#376L)\n",
      "            +- Scan ExistingRDD[PassengerId#376L,Name#377,Sex#378,Survived#379L]\n"
     ]
    }
   ],
   "source": [
    "#I can also checkpoint() after the first join to truncate the plan.\n",
    "df = df1.join(df1, ['PassengerId']).checkpoint()\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Partitions and repartition()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common cause of performance problems for me was having too many partitions. I think the Hadoop world call this the small file problem. A rule of thumb: keep the partitions to ~128MB.\n",
    "\n",
    "To check the number of partitions, use **.rdd.getNumPartitions()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe, despite having only 5 rows, has 4 partitions. This is too many. I can repartition to only 1 partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_repartitioned = df1.repartition(1)\n",
    "df1_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pixiedust\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/ba/7488f06b48238205562f9d63aaae2303c060c5dfd63b1ddd3bd9d4656eb1/pixiedust-1.1.18.tar.gz (197kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpld3 (from pixiedust)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798kB 15.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from pixiedust) (4.4.1)\n",
      "Collecting geojson (from pixiedust)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl\n",
      "Collecting astunparse (from pixiedust)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting markdown (from pixiedust)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 19.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colour (from pixiedust)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from pixiedust) (2.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from astunparse->pixiedust) (0.33.6)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from astunparse->pixiedust) (1.12.0)\n",
      "Requirement already satisfied: setuptools>=36 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from markdown->pixiedust) (41.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from requests->pixiedust) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from requests->pixiedust) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from requests->pixiedust) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erika/opt/anaconda3/lib/python3.7/site-packages (from requests->pixiedust) (2019.9.11)\n",
      "Building wheels for collected packages: pixiedust, mpld3\n",
      "  Building wheel for pixiedust (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pixiedust: filename=pixiedust-1.1.18-cp37-none-any.whl size=321728 sha256=7fe1a25e886f01f1f08c693188a94dd7e67cc22602f4d6df828c53d4b20efb36\n",
      "  Stored in directory: /Users/erika/Library/Caches/pip/wheels/e8/b1/86/c2f2e16e6bf9bfe556f9dbf8adb9f41816c476d73078c7d0eb\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116678 sha256=31332140382bf00f8f767f0e79f531d037e6c1bb62d787974bdf173cfb8e7528\n",
      "  Stored in directory: /Users/erika/Library/Caches/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "Successfully built pixiedust mpld3\n",
      "Installing collected packages: mpld3, geojson, astunparse, markdown, colour, pixiedust\n",
      "Successfully installed astunparse-1.6.3 colour-0.1.5 geojson-2.5.0 markdown-3.2.1 mpld3-0.3 pixiedust-1.1.18\n"
     ]
    }
   ],
   "source": [
    "!pip install pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !jupyter pixiedust install y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust will not collect anonymous install statistics.\n"
     ]
    }
   ],
   "source": [
    "pixiedust.optOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1=\"Hello\"\n",
    "var2=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cannot run scala code: SCALA_HOME environment variable not set\n"
     ]
    }
   ],
   "source": [
    "%%scala\n",
    "println(var1)\n",
    "println(var2 + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UDF functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* PySpark documentation [[Here]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframewriter#pyspark.sql.DataFrameWriter) \n",
    "* PySpark Dataframe Basics [[https://changhsinlee.com/pyspark-dataframe-basics/]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
