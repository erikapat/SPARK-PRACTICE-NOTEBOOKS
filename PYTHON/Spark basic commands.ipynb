{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spark tricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricks to avoid some problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use partitions**\n",
    "\n",
    "By default, when we perform a shuffle Spark will output two hundred shuffle partitions. We will set this value from 1 to five in order to reduce the number of the output partitions from the shuffle from two hundred to five.\n",
    "\n",
    "Go ahead and experiment with different values and see the number of partitions yourself. In experimenting with different values, you should see drastically different run times. Remenber that you can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.18 segundos\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# load data\n",
    "\n",
    "flightData2015 = spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"../data/2015-summary.csv\") \n",
    "\n",
    "\n",
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.07 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic dataframe terms**\n",
    "We define that we think are five basic verbs â€” select, filter, mutate, summarize, and arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one way\n",
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second way: sql expression\n",
    "df1.filter(\"Sex='female'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mutate**: creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summarize** using group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11fb555d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "gdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "|     1|              36.5|             62.2|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call multiple aggregation functions at once, pass a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "|     1|       2|              36.5|    124.4|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "|     1|     2|              36.5|     124.4|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\\\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Joins and unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join\n",
    "df1.join(df2, ['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unions\n",
    "#Union() returns a dataframe from the union of two dataframes\n",
    "df1.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common symptom of performance issues caused by chained unions in a for loop is it took longer and longer to iterate through the loop. In this case, **repartition()** and **checkpoint()** may help solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read save data**\n",
    "\n",
    "Check the reference: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframewriter#pyspark.sql.DataFrameWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The spark.sql API**\n",
    "\n",
    "Many of the operations that I showed can be accessed by writing SQL (Hive) queries in spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tuning performance or debugging dataframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cache a dataframe when it is used multiple times in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: bigint, Name: string, Sex: string, Survived: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we indicate that sometimes chaining too many union() cause performance problem or even out of memory errors. checkpoint() truncates the execution plan and saves the checkpointed dataframe to a temporary \n",
    "location on the disk.\n",
    "\n",
    "2.1. It is recomended caching before checkpointing, so Spark doesnâ€™t have to read in the dataframe from disk after itâ€™s checkpointed.\n",
    "\n",
    "2.2. To use checkpoint(), I need to specify the temporary file location to save the datafame to by accessing the sparkContext object from SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "#sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"checkpointdir\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://changhsinlee.com/pyspark-dataframe-basics/\n",
    "* https://medium.com/@gaga19900329/force-caching-spark-dataframes-84d32730a21\n",
    "* https://unraveldata.com/to-cache-or-not-to-cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
