{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spark tricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricks to avoid some problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use partitions**\n",
    "\n",
    "By default, when we perform a shuffle Spark will output two hundred shuffle partitions. We will set this value from 1 to five in order to reduce the number of the output partitions from the shuffle from two hundred to five.\n",
    "\n",
    "Go ahead and experiment with different values and see the number of partitions yourself. In experimenting with different values, you should see drastically different run times. Remenber that you can monitor the job progress by navigating to the Spark UI on port 4040 to see the physical and logical execution characteristics of our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.22 segundos\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# load data\n",
    "\n",
    "flightData2015 = spark\\\n",
    ".read\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"../data/2015-summary.csv\") \n",
    "\n",
    "\n",
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time require to run the model: 0.18 segundos\n"
     ]
    }
   ],
   "source": [
    "#time of execution for one partition\n",
    "timestart= datetime.datetime.now()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)\n",
    "\n",
    "# Calculation of the time\n",
    "timeend = datetime.datetime.now()\n",
    "timedelta = round((timeend-timestart).total_seconds(), 2) \n",
    "print(\"Time require to run the model: \" + str(timedelta) + \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic dataframe terms**\n",
    "We define that we think are five basic verbs â€” select, filter, mutate, summarize, and arrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
    "\n",
    "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|    Name|\n",
      "+-----------+--------+\n",
      "|          1|    Owen|\n",
      "|          2|Florence|\n",
      "|          3|   Laina|\n",
      "|          4|    Lily|\n",
      "|          5| William|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols1 = ['PassengerId', 'Name']\n",
    "df1.select(cols1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one way\n",
    "df1.filter(df1.Sex == 'female').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# second way: sql expression\n",
    "df1.filter(\"Sex='female'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## other way to filter strings\n",
    "df1.filter(df1.Sex.contains(\"female\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mutate**: creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+------------+\n",
      "|PassengerId|Age|Fare|Pclass|AgeTimesFare|\n",
      "+-----------+---+----+------+------------+\n",
      "|          1| 22| 7.3|     3|       160.6|\n",
      "|          2| 38|71.3|     1|      2709.4|\n",
      "|          3| 26| 7.9|     3|       205.4|\n",
      "|          4| 35|53.1|     1|      1858.5|\n",
      "|          5| 35| 8.0|     3|       280.0|\n",
      "+-----------+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summarize** using group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x11ce92ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf2 = df2.groupby('Pclass')\n",
    "gdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "|     1|              36.5|             62.2|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_cols = ['Age', 'Fare']\n",
    "gdf2.avg(*avg_cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call multiple aggregation functions at once, pass a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "|     1|       2|              36.5|    124.4|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "|     1|     2|              36.5|     124.4|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\\\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----+------+\n",
      "|PassengerId|Age|Fare|Pclass|\n",
      "+-----------+---+----+------+\n",
      "|          2| 38|71.3|     1|\n",
      "|          4| 35|53.1|     1|\n",
      "|          5| 35| 8.0|     3|\n",
      "|          3| 26| 7.9|     3|\n",
      "|          1| 22| 7.3|     3|\n",
      "+-----------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Fare', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Joins and unions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join\n",
    "df1.join(df2, ['PassengerId']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unions\n",
    "#Union() returns a dataframe from the union of two dataframes\n",
    "df1.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common symptom of performance issues caused by chained unions in a for loop is it took longer and longer to iterate through the loop. In this case, **repartition()** and **checkpoint()** may help solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The spark.sql API**\n",
    "\n",
    "Many of the operations that I showed can be accessed by writing SQL (Hive) queries in spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "#df.registerTempTable(\"connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select\n",
    "        a.PassengerId,\n",
    "        a.Name,\n",
    "        a.Sex,\n",
    "        a.Survived,\n",
    "        b.Age,\n",
    "        b.Fare,\n",
    "        b.Pclass\n",
    "    from df1_temp a\n",
    "    join df2_temp b\n",
    "        on a.PassengerId = b.PassengerId'''\n",
    "dfj = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "|          4|    Lily|female|       1| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1| 38|71.3|     1|\n",
      "|          5| William|  male|       0| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0| 22| 7.3|     3|\n",
      "+-----------+--------+------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create empty data.frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "field = [StructField('cod_pais_1',StringType(), True),\n",
    "        StructField('cod_entidad_1',IntegerType(), True),\n",
    "         StructField('cod_id_1',IntegerType(), True),\n",
    "         StructField('cod_persona_1',IntegerType(), True),\n",
    "         StructField('fec_movim_1',IntegerType(), True),\n",
    "         StructField('fec_month_1',IntegerType(), True),\n",
    "         StructField('partition_1',IntegerType(), True)\n",
    "        ]\n",
    "schema = StructType(field)\n",
    "#schema = cl_contratos_nomina.printSchema()\n",
    "table_name = sqlContext.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cod_pais_1: string (nullable = true)\n",
      " |-- cod_entidad_1: integer (nullable = true)\n",
      " |-- cod_id_1: integer (nullable = true)\n",
      " |-- cod_persona_1: integer (nullable = true)\n",
      " |-- fec_movim_1: integer (nullable = true)\n",
      " |-- fec_month_1: integer (nullable = true)\n",
      " |-- partition_1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **change the names of the data.frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_name_list= list(map(lambda x: x.replace(\"_1\", \"\"), table_name.columns))\n",
    "table_name = table_name.toDF(*new_column_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "|cod_pais|cod_entidad|cod_id|cod_persona|fec_movim|fec_month|partition|\n",
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "+--------+-----------+------+-----------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **schema with arrays**\n",
    "\n",
    "**Fisrt way**\n",
    "\n",
    "Sometime one of the columns is an array and we have to define those columns as arrays type.\n",
    "\n",
    "* 1st. Define shema before to load.\n",
    "* 2nd. Define columns that are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- imp_sdopost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_c0\",  StringType(), True),\n",
    "     StructField(\"imp_sdopost\",  StringType(), True)\n",
    "])\n",
    "\n",
    "trends = spark\\\n",
    ".read\\\n",
    ".schema(schema)\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"data/trends.csv\") \n",
    "\n",
    "trends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "trends= trends.withColumn(\"imp_sdopost\", split(col(\"imp_sdopost\"), \",\").cast(\"array<long>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- imp_sdopost: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|         imp_sdopost|\n",
      "+---+--------------------+\n",
      "|  0|[, 2915, 2912, 28...|\n",
      "|  1|[, 228, 228, 228,...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#(it does not work)\\nfrom pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\\nschema = StructType([\\n        StructField(\"_c0\", StringType(), True),\\n        StructField(\"imp_sdopost\", ArrayType(\\n            StructType([\\n                StructField(\"element\", StringType(), True),\\n            ])\\n        ), True)\\n    ])\\n\\ntrends_2 = spark.read.schema(schema).option(\"header\", \"true\").csv(\"data/trends.csv\") \\n\\n#trends_2.printSchema()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#(it does not work)\n",
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "schema = StructType([\n",
    "        StructField(\"_c0\", StringType(), True),\n",
    "        StructField(\"imp_sdopost\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"element\", StringType(), True),\n",
    "            ])\n",
    "        ), True)\n",
    "    ])\n",
    "\n",
    "trends_2 = spark\\\n",
    ".read\\\n",
    ".schema(schema)\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"data/trends.csv\") \n",
    "\n",
    "#trends_2.printSchema()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n#filter by row\\nfrom pyspark.sql.functions import lit,row_number,col\\nfrom pyspark.sql.window import Window\\ncols= [\\'cod_persctpn\\', \\'imp_sdopost\\']\\ntrends= trends.select(cols)\\n#select some rows\\nw = Window().partitionBy(lit(\\'a\\')).orderBy(lit(\\'a\\'))\\ntrends = trends.withColumn(\"row_num\", row_number().over(w))\\ntrends = trends.filter(col(\"row_num\").between(1,2))\\ntrends.select([\\'imp_sdopost\\']).toPandas().to_csv(\\'data/trends.csv\\')\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "#code used  to filter by row\n",
    "from pyspark.sql.functions import lit,row_number,col\n",
    "from pyspark.sql.window import Window\n",
    "cols= ['cod_persctpn', 'imp_sdopost']\n",
    "trends= trends.select(cols)\n",
    "#select some rows\n",
    "w = Window().partitionBy(lit('a')).orderBy(lit('a'))\n",
    "trends = trends.withColumn(\"row_num\", row_number().over(w))\n",
    "trends = trends.filter(col(\"row_num\").between(1,2))\n",
    "trends.select(['imp_sdopost']).toPandas().to_csv('data/trends.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **change squema after read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_rows = df_rows.select(\\n        df_rows.subT_DATE,\\n        df_rows.COUNT_USER.cast(\"float\"), \\n        df_rows.COUNT_NUM.cast(\"float\"), \\n        df_rows.SUMQ_TIME.cast(\"float\"),\\n        df_rows.SUMELP_TIME.cast(\"float\"), \\n        df_rows.SUMCPU_TIME.cast(\"float\")\\n    )\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_rows = df_rows.select(\n",
    "        df_rows.subT_DATE,\n",
    "        df_rows.COUNT_USER.cast(\"float\"), \n",
    "        df_rows.COUNT_NUM.cast(\"float\"), \n",
    "        df_rows.SUMQ_TIME.cast(\"float\"),\n",
    "        df_rows.SUMELP_TIME.cast(\"float\"), \n",
    "        df_rows.SUMCPU_TIME.cast(\"float\")\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save CSV**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#worst way\n",
    "flightData2015.write.format(\"csv\"). \\\n",
    "mode(\"overwrite\").options(header=\"true\",sep=\",\").save(path=\"data/data_csv\") #path to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other ways to do it\n",
    "# with repartition\n",
    "flightData2015.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    "   .mode(\"overwrite\").save(\"data/data_csv_2\") #path to folder\n",
    "\n",
    "# with coalesce\n",
    "flightData2015.coalesce(1).write.format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\").save(\"data/data_csv_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better and simplest way\n",
    "flightData2015.repartition(1).write.csv(\"data/test_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save and read Parquet format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.write.partitionBy(\"DEST_COUNTRY_NAME\").format(\"parquet\").save(\"data/flightData2015.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----+-----------------+\n",
      "|            Romania|   15|    United States|\n",
      "|            Croatia|    1|    United States|\n",
      "|            Ireland|  344|    United States|\n",
      "|              India|   62|    United States|\n",
      "|          Singapore|    1|    United States|\n",
      "+-------------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/flightData2015.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tuning performance or debugging dataframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cache a dataframe when it is used multiple times in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: bigint, Name: string, Sex: string, Survived: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we indicate that sometimes chaining too many union() cause performance problem or even out of memory errors. checkpoint() truncates the execution plan and saves the checkpointed dataframe to a temporary \n",
    "location on the disk.\n",
    "\n",
    "2.1. It is recomended caching before checkpointing, so Spark doesnâ€™t have to read in the dataframe from disk after itâ€™s checkpointed.\n",
    "\n",
    "2.2. To use checkpoint(), I need to specify the temporary file location to save the datafame to by accessing the sparkContext object from SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...\n",
    "#sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"checkpointdir\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(8) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#334, Sex#335, Survived#336L, Name#345, Sex#346, Survived#347L]\n",
      "+- *(8) SortMergeJoin [PassengerId#22L], [PassengerId#344L], Inner\n",
      "   :- *(5) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#334, Sex#335, Survived#336L]\n",
      "   :  +- *(5) SortMergeJoin [PassengerId#22L], [PassengerId#333L], Inner\n",
      "   :     :- *(2) Sort [PassengerId#22L ASC NULLS FIRST], false, 0\n",
      "   :     :  +- Exchange hashpartitioning(PassengerId#22L, 5)\n",
      "   :     :     +- *(1) Filter isnotnull(PassengerId#22L)\n",
      "   :     :        +- Scan ExistingRDD[PassengerId#22L,Name#23,Sex#24,Survived#25L]\n",
      "   :     +- *(4) Sort [PassengerId#333L ASC NULLS FIRST], false, 0\n",
      "   :        +- ReusedExchange [PassengerId#333L, Name#334, Sex#335, Survived#336L], Exchange hashpartitioning(PassengerId#22L, 5)\n",
      "   +- *(7) Sort [PassengerId#344L ASC NULLS FIRST], false, 0\n",
      "      +- ReusedExchange [PassengerId#344L, Name#345, Sex#346, Survived#347L], Exchange hashpartitioning(PassengerId#22L, 5)\n"
     ]
    }
   ],
   "source": [
    "#For example, I can join df1 to itself 3 times:\n",
    "df = df1.join(df1, ['PassengerId'])\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Project [PassengerId#22L, Name#23, Sex#24, Survived#25L, Name#359, Sex#360, Survived#361L, Name#377, Sex#378, Survived#379L]\n",
      "+- *(4) SortMergeJoin [PassengerId#22L], [PassengerId#376L], Inner\n",
      "   :- *(1) Filter isnotnull(PassengerId#22L)\n",
      "   :  +- Scan ExistingRDD[PassengerId#22L,Name#23,Sex#24,Survived#25L,Name#359,Sex#360,Survived#361L]\n",
      "   +- *(3) Sort [PassengerId#376L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PassengerId#376L, 5)\n",
      "         +- *(2) Filter isnotnull(PassengerId#376L)\n",
      "            +- Scan ExistingRDD[PassengerId#376L,Name#377,Sex#378,Survived#379L]\n"
     ]
    }
   ],
   "source": [
    "#I can also checkpoint() after the first join to truncate the plan.\n",
    "df = df1.join(df1, ['PassengerId']).checkpoint()\n",
    "df.join(df1, ['PassengerId']).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Partitions and repartition()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common cause of performance problems for me was having too many partitions. I think the Hadoop world call this the small file problem. A rule of thumb: keep the partitions to ~128MB.\n",
    "\n",
    "To check the number of partitions, use **.rdd.getNumPartitions()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe, despite having only 5 rows, has 4 partitions. This is too many. I can repartition to only 1 partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_repartitioned = df1.repartition(1)\n",
    "df1_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* PySpark documentation [[Here]](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframewriter#pyspark.sql.DataFrameWriter) \n",
    "* PySpark Dataframe Basics [[https://changhsinlee.com/pyspark-dataframe-basics/]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
