{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0},\n",
    "         'Age': {0: None, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'PassengerId2': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Name2': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
    "         'Sex2': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
    "         'Survived2': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0},\n",
    "         'Age2': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare2': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass2': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'Pclass3': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3},\n",
    "         'Pclass4': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}\n",
    "            }\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "data1 = spark.createDataFrame(df1_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 16\n"
     ]
    }
   ],
   "source": [
    "print(data1.count(), len(data1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def shape(self):\n",
    "    return (self.count(), len(self.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+----+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "|PassengerId|    Name|   Sex|Survived| Age|Fare|Pclass|PassengerId2|   Name2|  Sex2|Survived2|Age2|Fare2|Pclass2|Pclass3|Pclass4|\n",
      "+-----------+--------+------+--------+----+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "|          1|    Owen|  male|       0| NaN| 7.3|     3|           1|    Owen|  male|        0|  22|  7.3|      3|      3|      3|\n",
      "|          2|Florence|female|       1|38.0|71.3|     1|           2|Florence|female|        1|  38| 71.3|      1|      1|      1|\n",
      "|          3|   Laina|female|       1|26.0| 7.9|     3|           3|   Laina|female|        1|  26|  7.9|      3|      3|      3|\n",
      "|          4|    Lily|female|       1|35.0|53.1|     1|           4|    Lily|female|        1|  35| 53.1|      1|      1|      1|\n",
      "|          5| William|  male|       0|35.0| 8.0|     3|           5| William|  male|        0|  35|  8.0|      3|      3|      3|\n",
      "+-----------+--------+------+--------+----+----+------+------------+--------+------+---------+----+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning with PySpark With Natural Language Processing and Recommender Systems\n",
    "UDF's, page 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-853306ec0c1e>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-853306ec0c1e>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    parquet_uri='/home/jovyan/work/df_parquet'\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "write_uri= ' /home/jovyan/work/df_csv ' \n",
    "df.coalesce(1).write.format(\"csv\"). option(\"header\",\"true\").save(write_uri                                                             \n",
    "                                                                 \n",
    "parquet_uri='/home/jovyan/work/df_parquet'\n",
    "df.write.format('parquet').save(parquet_uri)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.VectorAssembler"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import vectorassembler to create dense vectors\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#create the vector assembler \n",
    "#handleInvalid = \"skip\"/keep\n",
    "vec_assmebler = VectorAssembler(inputCols=['PassengerId', 'Age'],\n",
    "                                outputCol='features', handleInvalid = \"keep\")\n",
    "type(vec_assmebler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the values\n",
    "features_df=vec_assmebler.transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Pclass: long (nullable = true)\n",
      " |-- PassengerId2: long (nullable = true)\n",
      " |-- Name2: string (nullable = true)\n",
      " |-- Sex2: string (nullable = true)\n",
      " |-- Survived2: long (nullable = true)\n",
      " |-- Age2: long (nullable = true)\n",
      " |-- Fare2: double (nullable = true)\n",
      " |-- Pclass2: long (nullable = true)\n",
      " |-- Pclass3: long (nullable = true)\n",
      " |-- Pclass4: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validate the presence of dense vectors \n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+\n",
      "|PassengerId| Age|  features|\n",
      "+-----------+----+----------+\n",
      "|          1| NaN| [1.0,NaN]|\n",
      "|          2|38.0|[2.0,38.0]|\n",
      "|          3|26.0|[3.0,26.0]|\n",
      "|          4|35.0|[4.0,35.0]|\n",
      "|          5|35.0|[5.0,35.0]|\n",
      "+-----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.select('PassengerId', 'Age', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
