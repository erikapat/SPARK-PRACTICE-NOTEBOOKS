{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHAPTER ONE: BASICS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿What is Apache Spark?\n",
    "\n",
    "\n",
    "It is an open source project encompassed within the **Hadoop ecosystem (although perhaps \n",
    "it should not).**\n",
    "\n",
    "It is a processing engine (not storage) based on MapReduce with the\n",
    "following features:\n",
    "\n",
    "- Based on the execution of in-memory and tasks on disk (instead of only on disk).\n",
    "- Developed in Scala but with “complete” interfaces for Java, Python and R.\n",
    "- Integrating multiple forms of work: batch, interactive, streaming ...\n",
    "- Supporting multiple architectures: standalone, execution on YARN ...\n",
    "\n",
    "\n",
    "It is the successor of “Hadoop MapReduce” primarily because of its:\n",
    "- Speed: thanks to memory execution and task optimization.\n",
    "- Ease of use: functions and programming models beyond MapReduce.\n",
    "- Flexibility: multiple languages, multiple architectures, multiple ways of working.\n",
    "- Homogeneity: a single framework for everything instead of an ecosystem.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark installation modes\n",
    "\n",
    "* Spark currently offers three different installation modes: standalone, cluster over\n",
    "Mesos, cluster on YARN.\n",
    "\n",
    "* Standalone:\n",
    "    \n",
    "  Installation mode with lower “configuration” needs (if you are not going to work in\n",
    "cluster).\n",
    "\n",
    "  > Ideal for development or work environments with “intermediate” volumes of information.\n",
    "  > Work oriented with \"local\" files.\n",
    "\n",
    "* Cluster over Mesos / YARN:\n",
    "    \n",
    "  > Installation mode with the greatest need for “configuration” (it is necessary to specify\n",
    "\"Manually\" cluster configuration).\n",
    "\n",
    "  > Ideal (and more than recommended) in production environments.\n",
    "  > Work oriented with \"distributed\" files.\n",
    "  \n",
    " ## TYPE OF OPERATIONS \n",
    " \n",
    " * map as transformation\n",
    " \n",
    " * reduce for action\n",
    " \n",
    " **Transformations** consisting of **narrow dependenciess** (we’ll call them narrow transformations) are those where each input partition will contribute to only one output partition.\n",
    " \n",
    " A wide dependency (or wide transformation) style transformation will have input partitions contributing to many\n",
    "output partitions. You will often hear this referred to as a shuffle where Spark will exchange partitions across the\n",
    "cluster. With narrow transformations, Spark will automatically perform an operation called pipelining on narrow\n",
    "dependencies, this means that if we specify multiple filters on DataFrames they’ll all be performed in-memory. The\n",
    "same cannot be said for shuffles. When we perform a shuffle, Spark will write the results to disk. You’ll see lots of talks\n",
    "about shuffle optimization across the web because it’s an important topic but for now all you need to understand are\n",
    "that there are two kinds of transformations\n",
    "\n",
    "## **Spark UI**\n",
    "During Spark’s execution of the previous code block, users can monitor the progress of their job through the Spark UI.\n",
    "The Spark UI is available on port 4040 of the driver node. If you are running in local mode this will just be the\n",
    "http://localhost:4040.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://github.com/XD-DENG/Spark-practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now, we can start with some code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'pyspark tutorial') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkContext parameters**\n",
    "\n",
    "* the driver (first argument) can be local[*], spark://”, **yarn, etc. What is available for you depends on how Spark has been deployed on the machine you use.\n",
    "\n",
    "* the second argument is the application name and is a human readable string you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "#To use a maximum of 2 tasks in parallel:\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[2]', 'pyspark tutorial') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to use all the available resource, you can simply use ‘*’ i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]', 'pyspark tutorial') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment of Spark:\n",
    "It can be deployed on:\n",
    "\n",
    "* a single machine such as your laptop (local)\n",
    "* a set of pre-defined machines (stand-alone)\n",
    "* a dedicated Hadoop-aware scheduler (YARN/Mesos)\n",
    "* “cloud”, e.g. Amazon EC2\n",
    "\n",
    "The development workflow is that you start small (local) and scale up to one of the other solutions, depending on your needs and resources.\n",
    "\n",
    "At UIO, we have the Abel cluster where Spark is available.\n",
    "\n",
    "Often, you don’t need to change any code to go between these methods of deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Components\n",
    "\n",
    "![alt text](fig/components.png \"Title\")\n",
    "\n",
    "* Spark Core: It is the core of Spark, the distributed execution engine in-memory.\n",
    "* Spark SQL: operations with SQL\n",
    "* Spark Streaming: real time processing\n",
    "* MLlib: machine learning\n",
    "* GraphX: graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OPERATIONS**\n",
    "\n",
    "* Map\n",
    "* Reduce\n",
    "* join\n",
    "* filter\n",
    "* sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **map/reduce**\n",
    "\n",
    "Let’s start, in this exercise the goal is to convert temperature from Celcius to Kelvin.\n",
    "\n",
    "Here it is how it translates in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.15, 276.15, 268.15, 298.15, 274.15, 282.15, 302.15, 263.15, 278.15]\n"
     ]
    }
   ],
   "source": [
    "temp_c = [10, 3, -5, 25, 1, 9, 29, -10, 5]\n",
    "rdd_temp_c = sc.parallelize(temp_c)\n",
    "rdd_temp_K = rdd_temp_c.map(lambda x: x + 273.15).collect()\n",
    "print(rdd_temp_K)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark:\n",
    "\n",
    "It is often a very bad idea to pull all the elements of the RDD to the driver because we potentially handle very large amount of data. So instead we prefer to use take as you can specify how many elements you wish to pull from the RDD.\n",
    "\n",
    "For instance to pull the first 3 elements only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[283.15, 276.15, 268.15]\n"
     ]
    }
   ],
   "source": [
    "temp_c = [10, 3, -5, 25, 1, 9, 29, -10, 5]\n",
    "rdd_temp_c = sc.parallelize(temp_c)\n",
    "rdd_temp_K = rdd_temp_c.map(lambda x: x + 273.15).take(3)\n",
    "print(rdd_temp_K)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "Now let’s take another example where we use **map as the transformation** and **reduce for the action** (types of operations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((1, (4, 6)), 2), (9, 10))\n"
     ]
    }
   ],
   "source": [
    "# we define a list of integers\n",
    "numbers = [1, 4, 6, 2, 9, 10]\n",
    "\n",
    "rdd_numbers=sc.parallelize(numbers)\n",
    "\n",
    "# Use reduce to combine numbers\n",
    "rdd_reduce = rdd_numbers.reduce(lambda x,y: \"(\" + str(x) + \", \" + str(y) + \")\")\n",
    "print(rdd_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda functions\n",
    "\n",
    "We see before a commad that is call lambda\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions defined without a name), using a construct called “lambda”.\n",
    "\n",
    "The general structure of a lambda function is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lambda <args>: <expr>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of normal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def f (x): \n",
    "    return x**2\n",
    "\n",
    "#For instance to use this function:\n",
    "print(f(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#The same function can be written as lambda function:\n",
    "\n",
    "g = lambda x: x**2\n",
    " \n",
    "# And you call it:\n",
    "print(g(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see both functions do exactly the same and can be used in the same ways.\n",
    "\n",
    "Note that the lambda definition does not include a “return” statement – it always contains a single expression which is returned.\n",
    "Also note that you can put a lambda definition anywhere a function is expected, and you don’t have to assign it to a variable at all.\n",
    "Lambda functions come from functional programming languages and the Lambda Calculus. Since they are so small they may be written on a single line.\n",
    "This is not exactly the same as lambda in functional programming languages, but it is a very powerful concept that’s well integrated into Python.\n",
    " \n",
    "\n",
    "## Conditional expression in Lambda functions\n",
    "You can use conditional expression in a lambda function or/and have more than one input argument.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PASS', 4, 50]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x,y: [\"PASS\",x,y] if x>3 and y<100 else [\"FAIL\",x,y]\n",
    "print(f(4,50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map\n",
    "Map takes a function f and an array as input parameters and outputs an array where f is applied to every element. \n",
    "In this respect, **using map is equivalent to for loops**.\n",
    "\n",
    "For instance, to convert a list of temperatures in Celsius to a list of temperature in Kelvin:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[283.15, 276.15, 268.15, 298.15, 274.15, 282.15, 302.15, 263.15, 278.15]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_c = [10, 3, -5, 25, 1, 9, 29, -10, 5]\n",
    "temp_K = list(map(lambda x: x + 273.15, temp_c))\n",
    "list(temp_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "map() is a function with two arguments:\n",
    "\n",
    "`r = map(func, seq)`\n",
    "\n",
    "The first argument func is the name of a function and the second a sequence (e.g. a list) \n",
    "seq. map() applies the function func to all the elements of the sequence seq. It returns a new list with the elements changed by func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Start by defining a variable pairs\n",
    "\n",
    "pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]\n",
    "\n",
    "Write a **Lambda function** and use it to sort pairs by key using their names. \n",
    "You will be using the list.sort() method of a list. It modifies the list in-place (here pairs)and has a key parameter to specify a function to be called on each list element prior to making comparisons. The value of the key parameter is a function that takes a single argument and returns a key to use for sorting purposes. Define this function as a Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]\n",
    "type(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]\n"
     ]
    }
   ],
   "source": [
    "pairs.sort(key=lambda pair: pair[1])\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "\n",
    "Let’s define a list of words: list_words = [“big”,”small”, “able”, “about”, “hairdresser”, “laboratory”]\n",
    "\n",
    "Use a **map function** to print the number of character of each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 4, 5, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "list_words = [\"big\", \"small\", \"able\", \"about\", \"hairdresser\", \"laboratory\"]\n",
    "print(list(map(len,list_words))) #more simple that it seems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "As the name suggests, **filter** can be used to filter your data. \n",
    "It tests each element of your input data and returns a subset of it for which a condition given by a function\n",
    "is TRUE. It does not modify your input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1]\n"
     ]
    }
   ],
   "source": [
    "numbers = range(-15, 15)\n",
    "less_than_zero = list(filter(lambda x: x < 0, numbers))\n",
    "print(less_than_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Reuse numbers and extract all the odd numbers:\n",
    "\n",
    "numbers = range(-15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(-15, 15)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = range(-15, 15)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-15, -13, -11, -9, -7, -5, -3, -1, 1, 3, 5, 7, 9, 11, 13]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x%2 ==1, numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce\n",
    "Reduce takes a function f and an array as input. The function f gets two \n",
    "input parameters that work on individual elements of the array. Reduce combines every \n",
    "two elements of the array using the function f. Let’s take an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 6, 2, 9, 10]\n",
      "[1, 4, 6, 2, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(((((1, 4), 6), 2), 9), 10)'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define a list of integers\n",
    "numbers = [1, 4, 6, 2, 9, 10]\n",
    "# Define a new function combine\n",
    "# Convert x and y to strings and create a tuple from x,y\n",
    "def combine(x,y):\n",
    "  return \"(\" + str(x) + \", \" + str(y) + \")\"\n",
    "\n",
    "# Use reduce to apply combine to numbers\n",
    "from functools import reduce\n",
    "\n",
    "print(numbers)\n",
    "reduce(combine,numbers)\n",
    "\n",
    "#To use the python reduce function, you need to import it from functools.\n",
    "#To define combine, we haven’t used a lambda function. With a Lambda function, we would have:\n",
    "# we define a list of integers\n",
    "numbers = [1, 4, 6, 2, 9, 10]\n",
    "\n",
    "# Use reduce to combine numbers\n",
    "from functools import reduce\n",
    "\n",
    "print(numbers)\n",
    "reduce(lambda x,y: \"(\" + str(x) + \", \" + str(y) + \")\",numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4\n",
    "\n",
    "Let’s define a string variable sentence:\n",
    "\n",
    "sentence = \"Dis-moi ce que tu manges, je te dirai ce que tu es.\"\n",
    "Compute the number of words in sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Challenge 4\n",
    "* First we remove punctuations from the sentence.\n",
    "* Then we split the resulting sentence and map 1 to each word of the sentence.\n",
    "*The last step is to sum up to find the number of words in the sentence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Dis-moi ce que tu manges, je te dirai ce que tu es.\"\n",
    "\n",
    "import string\n",
    "no_punctuation=sentence.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "reduce(lambda x,y: x+y, map(lambda x: 1, no_punctuation.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Apply it to an entire text to compute the total number of words of the text you upload yourself in your Galaxy \n",
    "history. Or use pre-loaded book available under the Data libraries available on the UIO Galaxy eduPortal \n",
    "(Share data –> Data Libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spark SQL**\n",
    "\n",
    "* Spark SQL is a component on top of Spark Core that facilitates processing of structured and semi-structured \n",
    "data and the integration of several data formats as source (Hive, Parquet, JSON).\n",
    "\n",
    "* It allows to transform RDDs using SQL (Structured Query Language).\n",
    "\n",
    "* To start Spark SQL within your notebook, you need to create a SQL context.\n",
    "\n",
    "* For this exercise, import a JSON file in a new history “World Cup”. You can find the historical World cup player dataset in JSON format in our Data Library named “Historical world cup player data “.\n",
    "\n",
    "* Then create a new python 3 (change kernel if set by default to python 2) jupyter notebook from this file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local', 'Spark SQL') \n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
