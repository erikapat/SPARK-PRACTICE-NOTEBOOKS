{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning and Advanced Analytics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "staticDataFrame: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n",
       "staticSchema: org.apache.spark.sql.types.StructType = StructType(StructField(InvoiceNo,StringType,true), StructField(StockCode,StringType,true), StructField(Description,StringType,true), StructField(Quantity,IntegerType,true), StructField(InvoiceDate,TimestampType,true), StructField(UnitPrice,DoubleType,true), StructField(CustomerID,DoubleType,true), StructField(Country,StringType,true))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val staticDataFrame = spark.read.format(\"csv\")\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .load(\"../data/retail-data/by-day/*.csv\")\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\") \n",
    "val staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Boolean = false\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staticDataFrame.isStreaming // returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.date_format\n",
       "preppedDataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "\n",
    "val preppedDataFrame = staticDataFrame \n",
    ".na.fill(0)\n",
    ".withColumn(\"day_of_week\", date_format($\"InvoiceDate\", \"EEEE\")) \n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 7 more fields]\n",
       "testDataFrame: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
    "val testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 245903\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 296006\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_de4beb49ddc5\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "val indexer = new StringIndexer() \n",
    ".setInputCol(\"day_of_week\") \n",
    ".setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_ea2d7b8458bb\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "val encoder = new OneHotEncoder() \n",
    ".setInputCol(\"day_of_week_index\") \n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_36ba342772da\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "val vectorAssembler = new VectorAssembler() \n",
    ".setInputCols(Array(\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\")) \n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIPELINES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "transformationPipeline: org.apache.spark.ml.Pipeline = pipeline_d1ead3f31d2f\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "val transformationPipeline = new Pipeline() \n",
    ".setStages(Array(indexer, encoder, vectorAssembler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fittedPipeline: org.apache.spark.ml.PipelineModel = pipeline_d1ead3f31d2f\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformedTraining: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: transformedTraining.type = [InvoiceNo: string, StockCode: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_887b4ae49487\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.KMeans\n",
    "val kmeans = new KMeans() \n",
    ".setK(20)\n",
    ".setSeed(1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmModel: org.apache.spark.ml.clustering.KMeansModel = kmeans_887b4ae49487\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Double = 1.0350348110517502E8\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmModel.computeCost(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformedTest: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MLlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Unable to infer schema for JSON. It must be specified manually.;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Unable to infer schema for JSON. It must be specified manually.;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:185)",
      "  at scala.Option.getOrElse(Option.scala:121)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:184)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)",
      "  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:325)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "var df = spark.read.json(\"../data/simple-ml\") \n",
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
